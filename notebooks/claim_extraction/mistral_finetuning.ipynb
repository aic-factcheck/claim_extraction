{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-12 16:31:03,615] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 16:31:05.167294: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-12 16:31:06.210969: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/appl/software/Python/3.10.4-GCCcore-11.3.0-bare/lib:/mnt/appl/software/libffi/3.4.2-GCCcore-11.3.0/lib64:/mnt/appl/software/XZ/5.2.5-GCCcore-11.3.0/lib:/mnt/appl/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/mnt/appl/software/Tcl/8.6.12-GCCcore-11.3.0/lib:/mnt/appl/software/libreadline/8.1.2-GCCcore-11.3.0/lib:/mnt/appl/software/ncurses/6.3-GCCcore-11.3.0/lib:/mnt/appl/software/bzip2/1.0.8-GCCcore-11.3.0/lib:/mnt/appl/software/binutils/2.38-GCCcore-11.3.0/lib:/mnt/appl/software/zlib/1.2.12-GCCcore-11.3.0/lib:/mnt/appl/software/GCCcore/11.3.0/lib64:/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib:/mnt/appl/software/GMP/6.2.1-GCCcore-11.3.0/lib\n",
      "2024-02-12 16:31:06.211068: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/appl/software/Python/3.10.4-GCCcore-11.3.0-bare/lib:/mnt/appl/software/libffi/3.4.2-GCCcore-11.3.0/lib64:/mnt/appl/software/XZ/5.2.5-GCCcore-11.3.0/lib:/mnt/appl/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/mnt/appl/software/Tcl/8.6.12-GCCcore-11.3.0/lib:/mnt/appl/software/libreadline/8.1.2-GCCcore-11.3.0/lib:/mnt/appl/software/ncurses/6.3-GCCcore-11.3.0/lib:/mnt/appl/software/bzip2/1.0.8-GCCcore-11.3.0/lib:/mnt/appl/software/binutils/2.38-GCCcore-11.3.0/lib:/mnt/appl/software/zlib/1.2.12-GCCcore-11.3.0/lib:/mnt/appl/software/GCCcore/11.3.0/lib64:/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib:/mnt/appl/software/GMP/6.2.1-GCCcore-11.3.0/lib\n",
      "2024-02-12 16:31:06.211074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-02-12 16:31:09,361\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.2 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-02-12 16:31:09,497\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.2 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import TrainingArguments\n",
    "import os, wandb\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load \"/mnt/data/factcheck/claim_extraction/feversum/train.jsonl\", \"/mnt/data/factcheck/claim_extraction/feversum/validation.jsonl\", \"/mnt/data/factcheck/claim_extraction/feversum/test.jsonl\"\n",
    "df_train = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf/train.jsonl\", lines=True)\n",
    "df_val = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf/validation.jsonl\", lines=True)\n",
    "df_test = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf/test.jsonl\", lines=True)\n",
    "#make datasetdict\n",
    "raw_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"validation\": Dataset.from_pandas(df_val),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Napoleon\\nHe won most of these wars and the vast majority of his battles, building a large empire that ruled over continental Europe before its final collapse in 1815.\\nOne of the greatest commanders in history, his wars and campaigns are studied at military schools worldwide.\\nNapoleon's political and cultural legacy has endured as one of the most celebrated and controversial leaders in human history.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][\"sentence_context\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Napoleon was a commander.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][\"claim\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_1 = \"\"\"Extract a single factual claim from the following Wikipedia sentence:\n",
    "{context}\n",
    "----\n",
    "You must print only the one claim extracted from this context, as a sentence that does not require additional context to interpret, printed as a single-line output.\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(datapoint, hide_output=False, format=FORMAT_1):\n",
    "    result = [\n",
    "        {\"role\": \"user\", \"content\": format.format(context=\"\\n\".join(datapoint[\"sentence_context\"]))}\n",
    "    ]\n",
    "    if not hide_output:\n",
    "        result.append({\"role\": \"assistant\", \"content\": datapoint[\"claim\"]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_prompt(raw_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3c61312eb54af091effe1e997f5de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "use_4bit = True\n",
    "   \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "device_map = \"auto\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,  # loading model in 4-bit\n",
    "    device_map=device_map, # to use max gpu resources if exist\n",
    ")\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1747004,\n",
       " 'source': 'Italy',\n",
       " 'sentence_id': 1747,\n",
       " 'claim': 'Italy had a civil war.',\n",
       " 'source_text': \"Italy\\nItaly (Italia [iÀàtaÀêlja]), officially the Italian Republic ([Repubblica italiana, links = no]), is a unitary parliamentary republic in Europe.The Italian peninsula is geographically located in Southern Europe, while North Italy can be placed partly or totally in Central Europe. Due to cultural, political and historical reasons, Italy is a Western European country. Located in the heart of the Mediterranean Sea, Italy shares open land borders with France, Switzerland, Austria, Slovenia, San Marino and Vatican City. Italy covers an area of 301338 km2 and has a largely temperate seasonal and Mediterranean climate. Due to its shape, it is often referred to in Italy as lo Stivale (the Boot). With 61 million inhabitants, it is the fourth most populous EU member state.\\nSince classical times, ancient Phoenicians, Carthaginians and Greeks established settlements in the south of Italy, with Etruscans and Celts inhabiting the centre and north of Italy respectively and various different ancient Italian tribes and Italic peoples dispersed throughout the Italian Peninsula and insular Italy. The Italic tribe known as the Latins formed the Roman Kingdom, which eventually became a republic that conquered and assimilated other nearby civilisations. Rome ultimately emerged as the dominant power in the Mediterranean basin, conquering much of the ancient world and becoming the leading cultural, political and religious centre of Western civilisation. The legacy of the Roman Empire is widespread and can be observed in the global distribution of civilian law, republican governments, Christianity and the Latin script.\\nDuring the Middle Ages, Italy suffered sociopolitical collapse amid calamitous barbarian invasions, but by the 11th century numerous rival city-states and maritime republics rose to great prosperity through shipping, commerce and banking, laying down the groundwork for modern capitalism. These independent statelets, acting as Europe's main trading hubs with Asia and the Near East, often enjoyed a greater degree of democracy and wealth in comparison to the larger feudal monarchies that were consolidating throughout Europe at the time, though much of central Italy remained under the control of the theocratic Papal States, while Southern Italy remained largely feudal until the 19th century, partially as a result of a succession of Byzantine, Arab, Norman, Spanish and Bourbon conquests of the region.\\nThe Renaissance began in Italy and spread to the rest of Europe, bringing a renewed interest in humanism, science, exploration and art. Italian culture flourished at this time, producing famous scholars, artists and polymaths such as Leonardo da Vinci, Galileo, Michelangelo and Machiavelli. Italian explorers such as Marco Polo, Christopher Columbus, Amerigo Vespucci and Giovanni da Verrazzano discovered new routes to the Far East and the New World, helping to usher in the European Age of Discovery. Nevertheless, Italy's commercial and political power significantly waned with the opening of the Atlantic trade route and the route to the Indian Ocean via the Cape of Good Hope, which bypassed the Mediterranean. Furthermore, the Italian city-states constantly engaged one another in bloody warfare, culminating in the Italian Wars of the 15th and 16th centuries that left them exhausted, with no one emerging as a dominant power. The weakened sovereigns soon fell victim to conquest by European powers such as France, Spain and Austria.\\nBy the mid-19th century, a rising movement in support of Italian nationalism and independence from foreign control led to a period of revolutionary political upheaval known as the Risorgimento, which sought the formation of a unified nation-state. After various unsuccessful attempts, the Italian Wars of Independence and the Expedition of the Thousand resulted in the eventual unification of the country in 1861, now a great power after centuries of foreign domination and political division. From the late 19th century to the early 20th century, the new Kingdom of Italy rapidly industrialised, although mainly in the north, and acquired a colonial empire, while the south remained largely impoverished and excluded from industrialisation, fuelling a large and influential diaspora. Despite being one of the main victors in World War I, Italy entered a period of economic crisis and social turmoil, leading the way to the rise of a fascist dictatorship in 1922. The subsequent participation in World War II on the Axis side ended in military defeat, economic destruction and an Italian civil war. Following the liberation of Italy and the rise of the resistance, the country abolished the monarchy, reinstated democracy, enjoyed a prolonged economic boom and, despite periods of sociopolitical turmoil (e.g. Anni di piombo, Mani pulite, the Second Mafia War, the Maxi Trial and subsequent assassinations of anti-mafia officials), became a major developed country.\\nToday, Italy has the third largest economy in the Eurozone and the eighth largest in the world. It has a very high level of human development and is ranked sixth in the world for life expectancy. The country plays a prominent role in regional and global economic, military, cultural and diplomatic affairs, and it is both a regional power and a great power. Italy is a founding and leading member of the European Union and the member of numerous international institutions, including the UN, NATO, the OECD, the OSCE, the WTO, the G7/G8, G20, the Union for the Mediterranean, the Council of Europe, Uniting for Consensus and many more. As a reflection of its cultural wealth, Italy is home to 51 World Heritage Sites, the most in the world, and is the fifth most visited country. \",\n",
       " 'sentence': 'The subsequent participation in World War II on the Axis side ended in military defeat, economic destruction and an Italian civil war.',\n",
       " 'sentence_context': 'Italy\\nDespite being one of the main victors in World War I, Italy entered a period of economic crisis and social turmoil, leading the way to the rise of a fascist dictatorship in 1922.\\nThe subsequent participation in World War II on the Axis side ended in military defeat, economic destruction and an Italian civil war.\\nFollowing the liberation of Italy and the rise of the resistance, the country abolished the monarchy, reinstated democracy, enjoyed a prolonged economic boom and, despite periods of sociopolitical turmoil (e.g. Anni di piombo, Mani pulite, the Second Mafia War, the Maxi Trial and subsequent assassinations of anti-mafia officials), became a major developed country.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][2911]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\n",
    "    \"\"\"Extract a single factual claim that follows from the following Wikipedia sentence:\n",
    "{context}\n",
    "----\n",
    "You must print only the one claim extracted from this context, as a sentence that does not require additional context to interpret, printed as a single-line output.\"\"\",\n",
    "    \"\"\"Article: {context}\n",
    "----\n",
    "You are an expert semanticist tasked to find all the facts mentioned in the context above. Print one factual claim per line, each a single sentence that does not require additional context be understood. Print as many lines as it takes to cover all the input facts.\"\"\",\n",
    "    \"\"\"You are an expert journalist assistant, you summarize the Article into a list of atomic factual claim it makes. I.e., your task is to enumerate all the most relevant claims this article makes. Please print one claim per line and make them interpretable on their own.\n",
    "Article: {context}\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Extract a single factual claim that follows from the following Wikipedia sentence:\\n2\\n4\\n \\n(\\n2\\n0\\n1\\n6\\n \\nf\\ni\\nl\\nm\\n)\\n\\n\\n2\\n4\\n \\ni\\ns\\n \\na\\n \\n2\\n0\\n1\\n6\\n \\nI\\nn\\nd\\ni\\na\\nn\\n \\nT\\na\\nm\\ni\\nl\\n-\\nl\\na\\nn\\ng\\nu\\na\\ng\\ne\\n \\ns\\nc\\ni\\ne\\nn\\nc\\ne\\n \\nf\\ni\\nc\\nt\\ni\\no\\nn\\n \\nt\\nh\\nr\\ni\\nl\\nl\\ne\\nr\\n \\nf\\ni\\nl\\nm\\n \\nw\\nr\\ni\\nt\\nt\\ne\\nn\\n \\na\\nn\\nd\\n \\nd\\ni\\nr\\ne\\nc\\nt\\ne\\nd\\n \\nb\\ny\\n \\nV\\ni\\nk\\nr\\na\\nm\\n \\nK\\nu\\nm\\na\\nr\\n.\\n\\n\\nB\\na\\ns\\ne\\nd\\n \\no\\nn\\n \\nt\\nh\\ne\\n \\nc\\no\\nn\\nc\\ne\\np\\nt\\n \\no\\nf\\n \\nt\\ni\\nm\\ne\\n-\\nt\\nr\\na\\nv\\ne\\nl\\n,\\n \\nt\\nh\\ne\\n \\nf\\ni\\nl\\nm\\n \\ns\\nt\\na\\nr\\ns\\n \\na\\nc\\nt\\no\\nr\\n \\nS\\nu\\nr\\ni\\ny\\na\\n \\ni\\nn\\n \\nt\\nr\\ni\\np\\nl\\ne\\n \\nr\\no\\nl\\ne\\ns\\n,\\n \\nw\\ni\\nt\\nh\\n \\na\\nc\\nt\\nr\\ne\\ns\\ns\\ne\\ns\\n \\nS\\na\\nm\\na\\nn\\nt\\nh\\na\\n \\nR\\nu\\nt\\nh\\n \\nP\\nr\\na\\nb\\nh\\nu\\n,\\n \\nN\\ni\\nt\\nh\\ny\\na\\n \\nM\\ne\\nn\\ne\\nn\\n \\na\\nn\\nd\\n \\nS\\na\\nr\\na\\nn\\ny\\na\\n \\nP\\no\\nn\\nv\\na\\nn\\nn\\na\\nn\\n \\ni\\nn\\n \\nl\\ne\\na\\nd\\n \\nr\\no\\nl\\ne\\ns\\n.\\n----\\nYou must print only the one claim extracted from this context, as a sentence that does not require additional context to interpret, printed as a single-line output.\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Generated Output: \n",
      "Vikram Kumar directed the film \"Indian Tamil-language film Thiruvithamizh\" (2001).\n",
      "\n",
      "---\n",
      "Expected Output: 24 is a 2016 film.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Article: 2\\n4\\n \\n(\\n2\\n0\\n1\\n6\\n \\nf\\ni\\nl\\nm\\n)\\n\\n\\n2\\n4\\n \\ni\\ns\\n \\na\\n \\n2\\n0\\n1\\n6\\n \\nI\\nn\\nd\\ni\\na\\nn\\n \\nT\\na\\nm\\ni\\nl\\n-\\nl\\na\\nn\\ng\\nu\\na\\ng\\ne\\n \\ns\\nc\\ni\\ne\\nn\\nc\\ne\\n \\nf\\ni\\nc\\nt\\ni\\no\\nn\\n \\nt\\nh\\nr\\ni\\nl\\nl\\ne\\nr\\n \\nf\\ni\\nl\\nm\\n \\nw\\nr\\ni\\nt\\nt\\ne\\nn\\n \\na\\nn\\nd\\n \\nd\\ni\\nr\\ne\\nc\\nt\\ne\\nd\\n \\nb\\ny\\n \\nV\\ni\\nk\\nr\\na\\nm\\n \\nK\\nu\\nm\\na\\nr\\n.\\n\\n\\nB\\na\\ns\\ne\\nd\\n \\no\\nn\\n \\nt\\nh\\ne\\n \\nc\\no\\nn\\nc\\ne\\np\\nt\\n \\no\\nf\\n \\nt\\ni\\nm\\ne\\n-\\nt\\nr\\na\\nv\\ne\\nl\\n,\\n \\nt\\nh\\ne\\n \\nf\\ni\\nl\\nm\\n \\ns\\nt\\na\\nr\\ns\\n \\na\\nc\\nt\\no\\nr\\n \\nS\\nu\\nr\\ni\\ny\\na\\n \\ni\\nn\\n \\nt\\nr\\ni\\np\\nl\\ne\\n \\nr\\no\\nl\\ne\\ns\\n,\\n \\nw\\ni\\nt\\nh\\n \\na\\nc\\nt\\nr\\ne\\ns\\ns\\ne\\ns\\n \\nS\\na\\nm\\na\\nn\\nt\\nh\\na\\n \\nR\\nu\\nt\\nh\\n \\nP\\nr\\na\\nb\\nh\\nu\\n,\\n \\nN\\ni\\nt\\nh\\ny\\na\\n \\nM\\ne\\nn\\ne\\nn\\n \\na\\nn\\nd\\n \\nS\\na\\nr\\na\\nn\\ny\\na\\n \\nP\\no\\nn\\nv\\na\\nn\\nn\\na\\nn\\n \\ni\\nn\\n \\nl\\ne\\na\\nd\\n \\nr\\no\\nl\\ne\\ns\\n.\\n----\\nYou are an expert semanticist tasked to find all the facts mentioned in the context above. Print one factual claim per line, each a single sentence that does not require additional context be understood. Print as many lines as it takes to cover all the input facts.\"\n",
      "    }\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Generated Output: \n",
      "1. Vikram Kumar is the director of the film.\n",
      "2. The film is called \"Indian Tamil-language film\".\n",
      "3. The film is a sci-fi fiction.\n",
      "4. The film is about a scientist.\n",
      "5. The film takes place in a tranquil village.\n",
      "6. The village is in Suriyana.\n",
      "7. The film has Menen, Prabhuhu, and Ruthtah as roles.\n",
      "8. The film has actors like Nithyasree, Samantha, and Saranya in it.\n",
      "9. The film is produced by Prabhuhu and Ruthtah.\n",
      "10. The film is penned by Menen.\n",
      "11. The film is written, directed, and directed by Vikram Kumar.\n",
      "\n",
      "---\n",
      "Expected Output: 24 is a 2016 film.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"You are an expert journalist assistant, you summarize the Article into a list of atomic factual claim it makes. I.e., your task is to enumerate all the most relevant claims this article makes. Please print one claim per line and make them interpretable on their own.\\nArticle: 2\\n4\\n \\n(\\n2\\n0\\n1\\n6\\n \\nf\\ni\\nl\\nm\\n)\\n\\n\\n2\\n4\\n \\ni\\ns\\n \\na\\n \\n2\\n0\\n1\\n6\\n \\nI\\nn\\nd\\ni\\na\\nn\\n \\nT\\na\\nm\\ni\\nl\\n-\\nl\\na\\nn\\ng\\nu\\na\\ng\\ne\\n \\ns\\nc\\ni\\ne\\nn\\nc\\ne\\n \\nf\\ni\\nc\\nt\\ni\\no\\nn\\n \\nt\\nh\\nr\\ni\\nl\\nl\\ne\\nr\\n \\nf\\ni\\nl\\nm\\n \\nw\\nr\\ni\\nt\\nt\\ne\\nn\\n \\na\\nn\\nd\\n \\nd\\ni\\nr\\ne\\nc\\nt\\ne\\nd\\n \\nb\\ny\\n \\nV\\ni\\nk\\nr\\na\\nm\\n \\nK\\nu\\nm\\na\\nr\\n.\\n\\n\\nB\\na\\ns\\ne\\nd\\n \\no\\nn\\n \\nt\\nh\\ne\\n \\nc\\no\\nn\\nc\\ne\\np\\nt\\n \\no\\nf\\n \\nt\\ni\\nm\\ne\\n-\\nt\\nr\\na\\nv\\ne\\nl\\n,\\n \\nt\\nh\\ne\\n \\nf\\ni\\nl\\nm\\n \\ns\\nt\\na\\nr\\ns\\n \\na\\nc\\nt\\no\\nr\\n \\nS\\nu\\nr\\ni\\ny\\na\\n \\ni\\nn\\n \\nt\\nr\\ni\\np\\nl\\ne\\n \\nr\\no\\nl\\ne\\ns\\n,\\n \\nw\\ni\\nt\\nh\\n \\na\\nc\\nt\\nr\\ne\\ns\\ns\\ne\\ns\\n \\nS\\na\\nm\\na\\nn\\nt\\nh\\na\\n \\nR\\nu\\nt\\nh\\n \\nP\\nr\\na\\nb\\nh\\nu\\n,\\n \\nN\\ni\\nt\\nh\\ny\\na\\n \\nM\\ne\\nn\\ne\\nn\\n \\na\\nn\\nd\\n \\nS\\na\\nr\\na\\nn\\ny\\na\\n \\nP\\no\\nn\\nv\\na\\nn\\nn\\na\\nn\\n \\ni\\nn\\n \\nl\\ne\\na\\nd\\n \\nr\\no\\nl\\ne\\ns\\n.\"\n",
      "    }\n",
      "]\n",
      "---\n",
      "Generated Output: \n",
      "1. The film \"24\" is a language-independent TV show.\n",
      "2. The film \"24\" is a scene-act structure.\n",
      "3. The film \"24\" is a real-time drama.\n",
      "4. The film \"24\" is set in Los Angeles, California.\n",
      "5. The film \"24\" is directed by Vikram Kumar.\n",
      "6. The film \"24\" is produced by Suriya, Prabhu Deva, and R. Madhavan.\n",
      "7. The film \"24\" is edited by Menon.\n",
      "8. The film \"24\" is a thriller.\n",
      "9. The film \"24\" is about a crisis that unfolds in real-time.\n",
      "10. The film \"24\" is about a series of events that\n",
      "\n",
      "---\n",
      "Expected Output: 24 is a 2016 film.\n",
      "\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample = raw_dataset[\"train\"][11]\n",
    "for prompt in [format_prompt(sample, format=el, hide_output=True) for el in formats]:\n",
    "    model_inputs = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # prettyprint json prompt\n",
    "    print(json.dumps(prompt, indent=4))\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        max_new_tokens=180, # set accordingly to your test_output\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids,skip_special_tokens=True)\n",
    "    # only preserve the output after last '[/INST]'\n",
    "    decoded_output = [el.split(\"[/INST]\")[-1].strip() for el in decoded_output]\n",
    "    # Output results for comparison\n",
    "    print(f\"---\\nGenerated Output: \\n{decoded_output[0]}\\n\")\n",
    "    print(f\"---\\nExpected Output: {sample['claim']}\\n\")\n",
    "    print(\"-\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Pick the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = DatasetDict(\n",
    "    {\n",
    "        k: Dataset.from_dict(\n",
    "            {\"text\": [tokenizer.apply_chat_template(format_prompt(el, format=formats[0]), tokenize=False) for el in v]}\n",
    "        )\n",
    "        for k, v in raw_dataset.items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] Extract a single factual claim that follows from the following Wikipedia sentence:\\nN\\na\\np\\no\\nl\\ne\\no\\nn\\n\\n\\nH\\ne\\n \\nw\\no\\nn\\n \\nm\\no\\ns\\nt\\n \\no\\nf\\n \\nt\\nh\\ne\\ns\\ne\\n \\nw\\na\\nr\\ns\\n \\na\\nn\\nd\\n \\nt\\nh\\ne\\n \\nv\\na\\ns\\nt\\n \\nm\\na\\nj\\no\\nr\\ni\\nt\\ny\\n \\no\\nf\\n \\nh\\ni\\ns\\n \\nb\\na\\nt\\nt\\nl\\ne\\ns\\n,\\n \\nb\\nu\\ni\\nl\\nd\\ni\\nn\\ng\\n \\na\\n \\nl\\na\\nr\\ng\\ne\\n \\ne\\nm\\np\\ni\\nr\\ne\\n \\nt\\nh\\na\\nt\\n \\nr\\nu\\nl\\ne\\nd\\n \\no\\nv\\ne\\nr\\n \\nc\\no\\nn\\nt\\ni\\nn\\ne\\nn\\nt\\na\\nl\\n \\nE\\nu\\nr\\no\\np\\ne\\n \\nb\\ne\\nf\\no\\nr\\ne\\n \\ni\\nt\\ns\\n \\nf\\ni\\nn\\na\\nl\\n \\nc\\no\\nl\\nl\\na\\np\\ns\\ne\\n \\ni\\nn\\n \\n1\\n8\\n1\\n5\\n.\\n\\n\\nO\\nn\\ne\\n \\no\\nf\\n \\nt\\nh\\ne\\n \\ng\\nr\\ne\\na\\nt\\ne\\ns\\nt\\n \\nc\\no\\nm\\nm\\na\\nn\\nd\\ne\\nr\\ns\\n \\ni\\nn\\n \\nh\\ni\\ns\\nt\\no\\nr\\ny\\n,\\n \\nh\\ni\\ns\\n \\nw\\na\\nr\\ns\\n \\na\\nn\\nd\\n \\nc\\na\\nm\\np\\na\\ni\\ng\\nn\\ns\\n \\na\\nr\\ne\\n \\ns\\nt\\nu\\nd\\ni\\ne\\nd\\n \\na\\nt\\n \\nm\\ni\\nl\\ni\\nt\\na\\nr\\ny\\n \\ns\\nc\\nh\\no\\no\\nl\\ns\\n \\nw\\no\\nr\\nl\\nd\\nw\\ni\\nd\\ne\\n.\\n\\n\\nN\\na\\np\\no\\nl\\ne\\no\\nn\\n'\\ns\\n \\np\\no\\nl\\ni\\nt\\ni\\nc\\na\\nl\\n \\na\\nn\\nd\\n \\nc\\nu\\nl\\nt\\nu\\nr\\na\\nl\\n \\nl\\ne\\ng\\na\\nc\\ny\\n \\nh\\na\\ns\\n \\ne\\nn\\nd\\nu\\nr\\ne\\nd\\n \\na\\ns\\n \\no\\nn\\ne\\n \\no\\nf\\n \\nt\\nh\\ne\\n \\nm\\no\\ns\\nt\\n \\nc\\ne\\nl\\ne\\nb\\nr\\na\\nt\\ne\\nd\\n \\na\\nn\\nd\\n \\nc\\no\\nn\\nt\\nr\\no\\nv\\ne\\nr\\ns\\ni\\na\\nl\\n \\nl\\ne\\na\\nd\\ne\\nr\\ns\\n \\ni\\nn\\n \\nh\\nu\\nm\\na\\nn\\n \\nh\\ni\\ns\\nt\\no\\nr\\ny\\n.\\n----\\nYou must print only the one claim extracted from this context, as a sentence that does not require additional context to interpret, printed as a single-line output. [/INST]Napoleon was a commander.</s>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"mistral_single_\" + os.environ[\"SLURM_JOB_ID\"]\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"/home/ullriher/ullriher/models/claim_extraction/paper/\" + run_name,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,  # Increase, if still giving OOM error\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=200,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,  # Enable fp16, bf16 only if your gfx card supports it\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=4.0,\n",
    "    weight_decay=0.001,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    run_name=run_name,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbertik\u001b[0m (\u001b[33maic_nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ullriher/ullriher/wandb/run-20240212_163745-wumqjrrh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper/runs/wumqjrrh' target=\"_blank\">mistral_single_7250872</a></strong> to <a href='https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper' target=\"_blank\">https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper/runs/wumqjrrh' target=\"_blank\">https://wandb.ai/aic_nlp/Mistral-Inst-7b-paper/runs/wumqjrrh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839637d8cfcc4b039cdae53fc648c3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411896d6e469477ab08a5f86628a123a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13620' max='13620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13620/13620 18:14:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0.280408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.270532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.267078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.268418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.268588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.274020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.271952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.273640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.277287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.283275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.283939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.292453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.295421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.294423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.301480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.298583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.311593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.322718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.323178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.324109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.321954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.334894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.330290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.334626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.334299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>0.331006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.334744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.343810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.340720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.333937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.338172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.346961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.350158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.340550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.368045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.372121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.377118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.379603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.369404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.374353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.371938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.374924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.373825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.375033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.371039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.394529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.382012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.398335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.381261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.387838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.392856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.419489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.418225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.416257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.413916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.417491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.440491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.432023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.451919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.457753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.443643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.443592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.432319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.433209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.439339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.439855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.444884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.444647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13620, training_loss=0.05675035911597169, metrics={'train_runtime': 65666.6418, 'train_samples_per_second': 0.83, 'train_steps_per_second': 0.207, 'total_flos': 2.1254752148693975e+18, 'train_loss': 0.05675035911597169, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Mistral-Inst-7b-paper\", name=run_name)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=prompt_dataset[\"train\"].shuffle(seed=42),\n",
    "    eval_dataset=prompt_dataset[\"validation\"].shuffle(seed=45),  # remove you have low VRAM and getting OOM errors\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4096,  # depends on your dataset\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes==0.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload bitsandbytes\n",
    "import importlib\n",
    "import bitsandbytes\n",
    "importlib.reload(bitsandbytes)\n",
    "bitsandbytes.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"][32999],df[\"summary\"][32999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "seg = pysbd.Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi! ', 'I am Bertie. ', 'How are You?']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg.segment(\"Hi! I am Bertie. How are You?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"aba\".split(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Extract a single factual claim that follows from the following Wikipedia sentence:\\nM\\ni\\nc\\nh\\na\\ne\\nl\\n \\nF\\na\\ns\\ns\\nb\\ne\\nn\\nd\\ne\\nr\\n\\n\\nM\\ni\\nc\\nh\\na\\ne\\nl\\n \\nF\\na\\ns\\ns\\nb\\ne\\nn\\nd\\ne\\nr\\n \\n(\\nb\\no\\nr\\nn\\n \\n2\\n \\nA\\np\\nr\\ni\\nl\\n \\n1\\n9\\n7\\n7\\n)\\n \\ni\\ns\\n \\na\\n \\nG\\ne\\nr\\nm\\na\\nn\\n-\\nb\\no\\nr\\nn\\n \\nI\\nr\\ni\\ns\\nh\\n \\na\\nc\\nt\\no\\nr\\n.\\n\\n\\nH\\ni\\ns\\n \\nf\\ne\\na\\nt\\nu\\nr\\ne\\n \\nf\\ni\\nl\\nm\\n \\nd\\ne\\nb\\nu\\nt\\n \\nw\\na\\ns\\n \\ni\\nn\\n \\nt\\nh\\ne\\n \\nf\\na\\nn\\nt\\na\\ns\\ny\\n \\nw\\na\\nr\\n \\ne\\np\\ni\\nc\\n \\n3\\n0\\n0\\n \\n(\\n2\\n0\\n0\\n7\\n)\\n \\na\\ns\\n \\na\\n \\nS\\np\\na\\nr\\nt\\na\\nn\\n \\nw\\na\\nr\\nr\\ni\\no\\nr\\n;\\n \\nh\\ni\\ns\\n \\ne\\na\\nr\\nl\\ni\\ne\\nr\\n \\nr\\no\\nl\\ne\\ns\\n \\ni\\nn\\nc\\nl\\nu\\nd\\ne\\nd\\n \\nv\\na\\nr\\ni\\no\\nu\\ns\\n \\ns\\nt\\na\\ng\\ne\\n \\np\\nr\\no\\nd\\nu\\nc\\nt\\ni\\no\\nn\\ns\\n,\\n \\na\\ns\\n \\nw\\ne\\nl\\nl\\n \\na\\ns\\n \\ns\\nt\\na\\nr\\nr\\ni\\nn\\ng\\n \\nr\\no\\nl\\ne\\ns\\n \\no\\nn\\n \\nt\\ne\\nl\\ne\\nv\\ni\\ns\\ni\\no\\nn\\n \\ns\\nu\\nc\\nh\\n \\na\\ns\\n \\ni\\nn\\n \\nt\\nh\\ne\\n \\nH\\nB\\nO\\n \\nm\\ni\\nn\\ni\\ns\\ne\\nr\\ni\\ne\\ns\\n \\nB\\na\\nn\\nd\\n \\no\\nf\\n \\nB\\nr\\no\\nt\\nh\\ne\\nr\\ns\\n \\n(\\n2\\n0\\n0\\n1\\n)\\n \\na\\nn\\nd\\n \\nt\\nh\\ne\\n \\nS\\nk\\ny\\n \\nO\\nn\\ne\\n \\nf\\na\\nn\\nt\\na\\ns\\ny\\n \\nd\\nr\\na\\nm\\na\\n \\nH\\ne\\nx\\n \\n(\\n2\\n0\\n0\\n4\\n \\n-\\n \\n0\\n5\\n)\\n.\\n----\\nYou must print only the one claim extracted from this context, as a sentence that does not require additional context to interpret, printed as a single-line output.'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(model\u001b[38;5;241m.\u001b[39mgenerate(model_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m sents \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mseg\u001b[49m\u001b[38;5;241m.\u001b[39msegment(output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,sents)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExpected:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,raw_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seg' is not defined"
     ]
    }
   ],
   "source": [
    "with open(training_arguments.output_dir+\"/generated_predictions.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "for i in tqdm(range(len(raw_dataset[\"test\"][:1]))):\n",
    "    eval_prompt = format_prompt(raw_dataset[\"test\"][i], format=formats[0], hide_output=True)\n",
    "    model_input = tokenizer.apply_chat_template(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(eval_prompt)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print()\n",
    "        output = tokenizer.decode(model.generate(model_input, max_new_tokens=100)[0], skip_special_tokens=True)\n",
    "        sents = [sent.strip() for sent in seg.segment(output.split(\"[/INST]\")[-1])]\n",
    "        print(\"\\nGenerated:\\n\",sents)\n",
    "        print(\"\\nExpected:\\n\",raw_dataset[\"test\"][i][\"claims\"])\n",
    "        with open(training_arguments.output_dir+\"/generated_predictions.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            print(json.dumps(sents),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = add_prompt(-1,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge()\n",
    "model.save_pretrained(\"merged_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "use_4bit = True\n",
    "   \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "device_map = \"auto\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,  # loading model in 4-bit\n",
    "    device_map=device_map, # to use max gpu resources if exist\n",
    ")\n",
    "\n",
    "#Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
