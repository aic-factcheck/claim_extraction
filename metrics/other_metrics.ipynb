{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "-> write as simple cells to be converted into scripts with args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decontextualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SavedModel in eager mode.\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
      "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     imported \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload(model_path, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserve\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: imported\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m\"\u001b[39m](tf\u001b[38;5;241m.\u001b[39mconstant(x))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 17\u001b[0m predict_fn \u001b[38;5;241m=\u001b[39m \u001b[43mload_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVED_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecontextualize\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict_fn([\u001b[38;5;28minput\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mload_predict_fn\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_predict_fn\u001b[39m(model_path):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading SavedModel in eager mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     imported \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserve\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: imported\u001b[38;5;241m.\u001b[39msignatures[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m\"\u001b[39m](tf\u001b[38;5;241m.\u001b[39mconstant(x))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:828\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[1;32m    827\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 828\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/saved_model/load.py:977\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModels saved from Tensorflow 1.x or Estimator (any\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    975\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m version) cannot be loaded with node filters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    976\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m--> 977\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mload_v1_in_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m     root\u001b[38;5;241m.\u001b[39mgraph_debug_info \u001b[38;5;241m=\u001b[39m debug_info\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filters:\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:284\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    282\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V1_V2_LABEL)\n\u001b[1;32m    283\u001b[0m loader \u001b[38;5;241m=\u001b[39m _EagerSavedModelLoader(export_dir)\n\u001b[0;32m--> 284\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m metrics\u001b[38;5;241m.\u001b[39mIncrementRead(write_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:254\u001b[0m, in \u001b[0;36m_EagerSavedModelLoader.load\u001b[0;34m(self, tags)\u001b[0m\n\u001b[1;32m    252\u001b[0m initializer \u001b[38;5;241m=\u001b[39m _Initializer(init_fn, asset_paths)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m local_init_op, _ \u001b[38;5;241m=\u001b[39m \u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:69\u001b[0m, in \u001b[0;36m_Initializer._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 69\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asset_paths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1474\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \n\u001b[1;32m   1427\u001b[0m \u001b[38;5;124;03m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/wrap_function.py:243\u001b[0m, in \u001b[0;36mWrappedFunction._call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_flat(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWrappedFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1492\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m structured_err\n\u001b[0;32m-> 1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_flat_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1541\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1536\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1537\u001b[0m       arg, (ops\u001b[38;5;241m.\u001b[39mTensor, resource_variable_ops\u001b[38;5;241m.\u001b[39mBaseResourceVariable)):\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_signature_summary()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: expected argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1539\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(zero-based) to be a Tensor; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1540\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import tensorflow as tf, pandas as pd\n",
    "import tensorflow_text  # Required to run exported model.\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_BUCKET = \"/mnt/personal/ullriher/models/tf/decontext_dataset\"\n",
    "SAVED_MODEL_PATH = path.join(\"/mnt/personal/ullriher/models/tf/decontext_dataset\", \"t5_base/1611267950\")\n",
    "\n",
    "\n",
    "def load_predict_fn(model_path):\n",
    "    print(\"Loading SavedModel in eager mode.\")\n",
    "    imported = tf.saved_model.load(model_path, [\"serve\"])\n",
    "    return lambda x: imported.signatures[\"serving_default\"](tf.constant(x))[\"outputs\"].numpy()\n",
    "\n",
    "predict_fn = load_predict_fn(SAVED_MODEL_PATH)\n",
    "\n",
    "def decontextualize(input):\n",
    "    return predict_fn([input])[0].decode(\"utf-8\")\n",
    "\n",
    "dp = {}\n",
    "def decontextualize_with_dp(input):\n",
    "    if input in dp:\n",
    "        return dp[input]\n",
    "    result = decontextualize(input)\n",
    "    dp[input] = result\n",
    "    return result\n",
    "\n",
    "def create_input(paragraph, target, page_title=\"\", section_title=\"\"):\n",
    "    prefix = paragraph\n",
    "    return \" [SEP] \".join((page_title, section_title, prefix, target, \"\"))\n",
    "\n",
    "def same_alphabetic_chars(generated, decontext_proposed):\n",
    "    a = \"\".join(filter(str.isalpha, generated)).lower()\n",
    "    b = \"\".join(filter(str.isalpha, decontext_proposed)).lower()\n",
    "    return  bool(len(a)) and a == b\n",
    "\n",
    "# if running from command line, parse argument as model name\n",
    "if False:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"model_name\", type=str, help=\"Name of the model to use\")\n",
    "    args = parser.parse_args()\n",
    "    model_name = args.model_name\n",
    "else:\n",
    "    model_name = \"t5_small_multiclaim\"\n",
    "\n",
    "df = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf_multiclaim/test.jsonl\", lines=True)\n",
    "df[\"generated\"]=None\n",
    "predictions = f\"/home/ullriher/ullriher/data/_paper/predictions/{model_name}.jsonl\"\n",
    "outfile = f\"/home/ullriher/ullriher/data/_paper/metrics/decontextualization/{model_name}.jsonl\"\n",
    "skip_title_from_context = True\n",
    "\n",
    "with open(predictions, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        df.at[i, \"generated\"] = json.loads(line)\n",
    "        # if context starts with source\\n, remove it\n",
    "        if skip_title_from_context:\n",
    "            if df.at[i, \"sentence_context\"].startswith(df.at[i, \"source\"]+\"\\n\"):\n",
    "                df.at[i, \"sentence_context\"] = df.at[i, \"sentence_context\"][len(df.at[i, \"source\"])+1:]\n",
    "\n",
    "# expand df by generated, one row per element of generated list\n",
    "df = df.explode(\"generated\").reset_index(drop=True)\n",
    "df.drop(columns=[\"source_text\"], inplace=True)\n",
    "# remove leading source\\n from sentence_context\n",
    "df[\"decontext_result\"] = None\n",
    "df[\"decontext_label\"] = None\n",
    "df[\"decontext_proposed\"] = None\n",
    "\n",
    "if path.exists(outfile):\n",
    "    df = pd.read_json(outfile, lines=True)\n",
    "    print(f\"Loaded checkpoint from {outfile}\")\n",
    "\n",
    "# if claims column, drop it\n",
    "if \"claims\" in df.columns:\n",
    "    df.drop(columns=[\"claims\"], inplace=True)\n",
    "    \n",
    "print(\"predicting\")\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row[\"decontext_label\"] is not None:\n",
    "        continue\n",
    "    \n",
    "    page_title = row[\"source\"]\n",
    "    section_title = \"\"\n",
    "    input = create_input(row[\"sentence_context\"], row[\"generated\"], page_title, section_title)\n",
    "    decontextualized = decontextualize(input)\n",
    "    row[\"decontext_label\"], row[\"decontext_proposed\"] = [s.strip() for s in decontextualized.split(\"####\", 1)]\n",
    "    \n",
    "    if same_alphabetic_chars(row[\"generated\"], row[\"decontext_proposed\"]):\n",
    "        row[\"decontext_label\"] = \"UNNECESSARY\"\n",
    "        \n",
    "    # only preserve alphabetic and turn to uppercase\n",
    "    df.at[index, \"decontext_result\"] = decontextualized\n",
    "    df.at[index, \"decontext_label\"] = \"\".join(filter(str.isalpha, row[\"decontext_label\"])).upper()\n",
    "    df.at[index, \"decontext_proposed\"] = row[\"decontext_proposed\"]\n",
    "    # break at 100\n",
    "    # save df to outfile\n",
    "    if True or index % 100 == 0:\n",
    "        df.to_json(outfile, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /home/ullriher/ullriher/data/_paper/metrics/decontextualization/t5_small_multiclaim.jsonl\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 14: ml: not found\n",
      "sh: 15: module: not found\n",
      "sh: 17: source: not found\n",
      "2024-02-14 01:04:40.777072: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/appl/software/Python/3.10.4-GCCcore-11.3.0-bare/lib:/mnt/appl/software/libffi/3.4.2-GCCcore-11.3.0/lib64:/mnt/appl/software/XZ/5.2.5-GCCcore-11.3.0/lib:/mnt/appl/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/mnt/appl/software/Tcl/8.6.12-GCCcore-11.3.0/lib:/mnt/appl/software/libreadline/8.1.2-GCCcore-11.3.0/lib:/mnt/appl/software/ncurses/6.3-GCCcore-11.3.0/lib:/mnt/appl/software/bzip2/1.0.8-GCCcore-11.3.0/lib:/mnt/appl/software/binutils/2.38-GCCcore-11.3.0/lib:/mnt/appl/software/zlib/1.2.12-GCCcore-11.3.0/lib:/mnt/appl/software/GCCcore/11.3.0/lib64:/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib:/mnt/appl/software/GMP/6.2.1-GCCcore-11.3.0/lib\n",
      "2024-02-14 01:04:40.777171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/appl/software/Python/3.10.4-GCCcore-11.3.0-bare/lib:/mnt/appl/software/libffi/3.4.2-GCCcore-11.3.0/lib64:/mnt/appl/software/XZ/5.2.5-GCCcore-11.3.0/lib:/mnt/appl/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/mnt/appl/software/Tcl/8.6.12-GCCcore-11.3.0/lib:/mnt/appl/software/libreadline/8.1.2-GCCcore-11.3.0/lib:/mnt/appl/software/ncurses/6.3-GCCcore-11.3.0/lib:/mnt/appl/software/bzip2/1.0.8-GCCcore-11.3.0/lib:/mnt/appl/software/binutils/2.38-GCCcore-11.3.0/lib:/mnt/appl/software/zlib/1.2.12-GCCcore-11.3.0/lib:/mnt/appl/software/GCCcore/11.3.0/lib64:/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib:/mnt/appl/software/GMP/6.2.1-GCCcore-11.3.0/lib\n",
      "2024-02-14 01:04:40.777180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-14 01:04:49,285] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Computing metric: atomicity\n",
      "Using model qacg\n",
      "predicting\n",
      "Loading Named Entity Recognition Pipeline\u001b[33m...\u001b[0m\n",
      "2024-02-14 01:05:11,768 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC\n",
      "Loading Relation Extraction Pipeline\u001b[33m...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:53,  5.38s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ullriher/ullriher/src/metric_atomicity.py\", line 94, in <module>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    df.at[index, \"rebel_facts\"] = rebel_solve(row[\"generated\"])\n",
      "  File \"/home/ullriher/ullriher/src/metric_atomicity.py\", line 24, in rebel_solve\n",
      "    preds = rebel(input_text, return_tensors=True, return_text=False)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 167, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1162, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1169, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1068, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 191, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1558, in generate\n",
      "    return self.beam_search(\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2942, in beam_search\n",
      "    outputs = self(\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1731, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1617, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 1470, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py\", line 796, in forward\n",
      "    hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/activations.py\", line 79, in forward\n",
      "    return self.act(input)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --partition amdgpu\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --ntasks-per-node 1\n",
    "#SBATCH --mem-per-cpu 64G\n",
    "#SBATCH --gres gpu:1\n",
    "#SBATCH --time 24:00:00\n",
    "#SBATCH --job-name acl_feversum\n",
    "#SBATCH --output /home/ullriher/ullriher/logs/expts/amdsum.%j.out\n",
    "\n",
    "\n",
    "\n",
    "ml Python/3.10.4-GCCcore-11.3.0-bare\n",
    "module unload OpenSSL/1.1\n",
    "\n",
    "source ~/venvs/2023feb/bin/activate\n",
    "cd ~/ullriher/src\n",
    "\n",
    "export PYTHONPATH=/home/ullriher/ullriher/src:$PYTHONPATH\n",
    "export PATH=/home/ullriher/nodejs-latest/node-v15.14.0:/home/ullriher/venv_amd/bin:$PATH\n",
    "\n",
    "#deepspeed --num_gpus=1 \n",
    "~/venvs/2023feb/bin/python metric_atomicity.py qacg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop where generated is null\n",
    "df = df.dropna(subset=[\"generated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_context</th>\n",
       "      <th>generated</th>\n",
       "      <th>rebel</th>\n",
       "      <th>factsumm</th>\n",
       "      <th>rebel_facts</th>\n",
       "      <th>factsumm_facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.</td>\n",
       "      <td>In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...</td>\n",
       "      <td>R. Kelly has been a guest vocalist for Nas.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(R. Kelly, member of, Nas), (Nas, has part, R. Kelly)]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.</td>\n",
       "      <td>In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...</td>\n",
       "      <td>R. Kelly has been a guest vocalist for Sean Combs.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(R. Kelly, member of, Sean Combs), (Sean Combs, has part, R. Kelly)]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.</td>\n",
       "      <td>In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...</td>\n",
       "      <td>R. Kelly has been a guest vocalist for The Notorious B.I.G.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(R. Kelly, member of, The Notorious B.I.G.), (The Notorious B.I.G., has part, R. Kelly)]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.</td>\n",
       "      <td>In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...</td>\n",
       "      <td>R. Kelly has been recognized by the Recording Industry Association of America.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[(R. Kelly, member of, Recording Industry Association of America)]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.</td>\n",
       "      <td>In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...</td>\n",
       "      <td>R. Kelly has been recognized as one of the best-selling music artists in the United States.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[(R. Kelly, country of citizenship, United States)]</td>\n",
       "      <td>{(R. Kelly, per:countries_of_residence, the United States), (one, per:countries_of_residence, the United States)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).</td>\n",
       "      <td>Adrianne Palicki was born in 1983.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).</td>\n",
       "      <td>Adrianne Palicki is an American.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).</td>\n",
       "      <td>Adrianne Palicki is best known for her role as Tyra Collette.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).</td>\n",
       "      <td>Adrianne Palicki is best known for her role in Legion.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).</td>\n",
       "      <td>Adrianne Palicki is best known for her role in Red Dawn.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1977 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                source  sentence_id  \\\n",
       "0             R. Kelly         6140   \n",
       "1             R. Kelly         6140   \n",
       "2             R. Kelly         6140   \n",
       "3             R. Kelly         6140   \n",
       "4             R. Kelly         6140   \n",
       "...                ...          ...   \n",
       "1972  Adrianne Palicki         3541   \n",
       "1973  Adrianne Palicki         3541   \n",
       "1974  Adrianne Palicki         3541   \n",
       "1975  Adrianne Palicki         3541   \n",
       "1976  Adrianne Palicki         3541   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                  sentence  \\\n",
       "0     In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.   \n",
       "1     In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.   \n",
       "2     In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.   \n",
       "3     In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.   \n",
       "4     In 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the same list.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "1972                                                                                                                                             Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).   \n",
       "1973                                                                                                                                             Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).   \n",
       "1974                                                                                                                                             Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).   \n",
       "1975                                                                                                                                             Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).   \n",
       "1976                                                                                                                                             Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     sentence_context  \\\n",
       "0     In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...   \n",
       "1     In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...   \n",
       "2     In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...   \n",
       "3     In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...   \n",
       "4     In 1996, Kelly was nominated for a Grammy for writing Michael Jackson's song \"You Are Not Alone\".\\nIn 2002 and 2004, Kelly released collaboration albums with rapper Jay-Z and has been a guest vocalist for other hip hop artists like Nas, Sean Combs, and The Notorious B.I.G.   The Recording Industry Association of America (RIAA) has recognized R. Kelly as one of the best-selling music artists in the United States with 40 million albums sold as well as only the fifth black artist to crack the top 50 of the...   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n",
       "1972                                                                                                                                              Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).   \n",
       "1973                                                                                                                                              Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).   \n",
       "1974                                                                                                                                              Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).   \n",
       "1975                                                                                                                                              Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).   \n",
       "1976                                                                                                                                              Adrianne Lee Palicki (born May 6, 1983) is an American actress best known for her roles as Tyra Collette in the television series Friday Night Lights (2006 - 2011) and supporting roles in the films Legion (2010), Red Dawn (2012), G.I. Joe: Retaliation (2013), and John Wick (2014).\\nShe played Barbara \"Bobbi\" Morse on the ABC series Agents of S.H.I.E.L.D. (2014 - 2016).   \n",
       "\n",
       "                                                                                        generated  \\\n",
       "0                                                     R. Kelly has been a guest vocalist for Nas.   \n",
       "1                                              R. Kelly has been a guest vocalist for Sean Combs.   \n",
       "2                                     R. Kelly has been a guest vocalist for The Notorious B.I.G.   \n",
       "3                  R. Kelly has been recognized by the Recording Industry Association of America.   \n",
       "4     R. Kelly has been recognized as one of the best-selling music artists in the United States.   \n",
       "...                                                                                           ...   \n",
       "1972                                                           Adrianne Palicki was born in 1983.   \n",
       "1973                                                             Adrianne Palicki is an American.   \n",
       "1974                                Adrianne Palicki is best known for her role as Tyra Collette.   \n",
       "1975                                       Adrianne Palicki is best known for her role in Legion.   \n",
       "1976                                     Adrianne Palicki is best known for her role in Red Dawn.   \n",
       "\n",
       "     rebel factsumm  \\\n",
       "0        1        1   \n",
       "1        1        1   \n",
       "2        1        1   \n",
       "3        1        1   \n",
       "4        1        2   \n",
       "...    ...      ...   \n",
       "1972  None     None   \n",
       "1973  None     None   \n",
       "1974  None     None   \n",
       "1975  None     None   \n",
       "1976  None     None   \n",
       "\n",
       "                                                                                    rebel_facts  \\\n",
       "0                                       [(R. Kelly, member of, Nas), (Nas, has part, R. Kelly)]   \n",
       "1                         [(R. Kelly, member of, Sean Combs), (Sean Combs, has part, R. Kelly)]   \n",
       "2     [(R. Kelly, member of, The Notorious B.I.G.), (The Notorious B.I.G., has part, R. Kelly)]   \n",
       "3                            [(R. Kelly, member of, Recording Industry Association of America)]   \n",
       "4                                           [(R. Kelly, country of citizenship, United States)]   \n",
       "...                                                                                         ...   \n",
       "1972                                                                                       None   \n",
       "1973                                                                                       None   \n",
       "1974                                                                                       None   \n",
       "1975                                                                                       None   \n",
       "1976                                                                                       None   \n",
       "\n",
       "                                                                                                         factsumm_facts  \n",
       "0                                                                                                                    {}  \n",
       "1                                                                                                                    {}  \n",
       "2                                                                                                                    {}  \n",
       "3                                                                                                                    {}  \n",
       "4     {(R. Kelly, per:countries_of_residence, the United States), (one, per:countries_of_residence, the United States)}  \n",
       "...                                                                                                                 ...  \n",
       "1972                                                                                                               None  \n",
       "1973                                                                                                               None  \n",
       "1974                                                                                                               None  \n",
       "1975                                                                                                               None  \n",
       "1976                                                                                                               None  \n",
       "\n",
       "[1977 rows x 9 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch /home/ullriher/ullriher/slurm/tmp/1.batch\n",
      "sbatch /home/ullriher/ullriher/slurm/tmp/2.batch\n",
      "sbatch /home/ullriher/ullriher/slurm/tmp/3.batch\n",
      "sbatch /home/ullriher/ullriher/slurm/tmp/4.batch\n",
      "sbatch /home/ullriher/ullriher/slurm/tmp/5.batch\n"
     ]
    }
   ],
   "source": [
    "batch_folder = \"/home/ullriher/ullriher/slurm/tmp\"\n",
    "metric = \"qags\"\n",
    "i=1\n",
    "for model in ['qacg', 'qlora-mistral-instruct-v0.2','gpt-4-turbo-3-shot', 't5_small_multiclaim','t5_small_diverse_7_beam_search']:\n",
    "                with open(f\"{batch_folder}/{i}.batch\",\"w\") as f:\n",
    "                    print(f\"\"\"#!/bin/bash\n",
    "#SBATCH --partition amdgpu\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --ntasks-per-node 1\n",
    "#SBATCH --mem-per-cpu 64G\n",
    "#SBATCH --gres gpu:1\n",
    "#SBATCH --time 24:00:00\n",
    "#SBATCH --job-name  {metric}\n",
    "#SBATCH --output /home/ullriher/ullriher/logs/metrics/amdsum.%j.out\n",
    "\n",
    "\n",
    "\n",
    "ml Python/3.10.4-GCCcore-11.3.0-bare\n",
    "module unload OpenSSL/1.1\n",
    "\n",
    "source ~/venvs/2023feb/bin/activate\n",
    "cd ~/ullriher/src\n",
    "\n",
    "export PYTHONPATH=/home/ullriher/ullriher/src:$PYTHONPATH\n",
    "export PATH=/home/ullriher/nodejs-latest/node-v15.14.0:/home/ullriher/venv_amd/bin:$PATH\n",
    "\n",
    "#deepspeed --num_gpus=1 \n",
    "~/venvs/2023feb/bin/python metric_{metric}.py {model}\"\"\",file=f)\n",
    "                    print(f\"sbatch {batch_folder}/{i}.batch\")\n",
    "                    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.datautils' from '/home/ullriher/ullriher/src/utils/datautils.py'>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use importlib and reload utils.datautils.extract_triplets\n",
    "import importlib\n",
    "import utils.datautils\n",
    "importlib.reload(utils.datautils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 32/32 [00:00<00:00, 65.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from alignscore import AlignScore\n",
    "\n",
    "scorer = AlignScore(model='roberta-base', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "score = scorer.score(contexts=['hello world.'], claims=['hello world.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.9947293400764465,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673,\n",
       " 0.013116993941366673]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.0.post1 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-large.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:255: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: faithfulness\n",
      "Using model t5_small_multiclaim\n",
      "Loaded checkpoint from /home/ullriher/ullriher/data/_paper/metrics/faithfulness/t5_small_multiclaim.jsonl\n",
      "predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.32it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.61it/s]\n",
      "20it [00:02,  8.73it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.31it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.47it/s]\n",
      "21it [00:04,  3.93it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 44.55it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.61it/s]\n",
      "22it [00:06,  2.33it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.57it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.59it/s]\n",
      "23it [00:09,  1.58it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.11it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.54it/s]\n",
      "24it [00:11,  1.19it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.71it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.59it/s]\n",
      "25it [00:13,  1.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.80it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.56it/s]\n",
      "26it [00:15,  1.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.46it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.43it/s]\n",
      "27it [00:17,  1.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 46.21it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.63it/s]\n",
      "28it [00:19,  1.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 44.63it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.59it/s]\n",
      "29it [00:21,  1.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.26it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.67it/s]\n",
      "30it [00:23,  1.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.62it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.54it/s]\n",
      "31it [00:26,  1.89s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 45.81it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.59it/s]\n",
      "32it [00:28,  1.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 46.50it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.67it/s]\n",
      "33it [00:30,  1.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 46.40it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.63it/s]\n",
      "34it [00:32,  1.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 43.74it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.48it/s]\n",
      "35it [00:34,  2.03s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 44.69it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.53it/s]\n",
      "36it [00:36,  2.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 46.05it/s]\n",
      "Evaluating: 100%|| 1/1 [00:00<00:00, 17.60it/s]\n",
      "37it [00:41,  2.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "37it [00:43,  1.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m claim \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     81\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, bert_score\u001b[38;5;241m.\u001b[39mscore([premise], [claim], model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m---> 82\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertscore_avgtop2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbertscore_solve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignscore_base\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m align_base\u001b[38;5;241m.\u001b[39mscore(contexts\u001b[38;5;241m=\u001b[39m[premise], claims\u001b[38;5;241m=\u001b[39m[claim])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     84\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignscore_large\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m align_large\u001b[38;5;241m.\u001b[39mscore(contexts\u001b[38;5;241m=\u001b[39m[premise], claims\u001b[38;5;241m=\u001b[39m[claim])[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[148], line 25\u001b[0m, in \u001b[0;36mbertscore_solve\u001b[0;34m(premise, claim)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbertscore_solve\u001b[39m(premise, claim):\n\u001b[1;32m     24\u001b[0m     text_sentencewise \u001b[38;5;241m=\u001b[39m sent_tokenize(premise)\n\u001b[0;32m---> 25\u001b[0m     P, R, F \u001b[38;5;241m=\u001b[39m \u001b[43mbert_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclaim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_sentencewise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_sentencewise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroberta-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mfloat\u001b[39m(avg_top_n(P, \u001b[38;5;241m2\u001b[39m)),\u001b[38;5;28mfloat\u001b[39m(avg_top_n(R, \u001b[38;5;241m2\u001b[39m)),\u001b[38;5;28mfloat\u001b[39m(avg_top_n(F, \u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/bert_score/score.py:97\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_layers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     num_layers \u001b[38;5;241m=\u001b[39m model2layers[model_type]\n\u001b[0;32m---> 97\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(model_type, num_layers, all_layers)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/bert_score/utils.py:329\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m(model_type, use_fast)\u001b[0m\n\u001b[1;32m    326\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m cache_scibert(model_type)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(trans_version) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_fast, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast tokenizer is not available for version < 4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:835\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 835\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    838\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         )\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1952\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 1952\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1966\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1967\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1968\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   1969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    729\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/urllib3/connectionpool.py:462\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import bert_score\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from utils.datautils import extract_triplets\n",
    "from transformers import pipeline\n",
    "from utils.ntbutils import load_user_libs\n",
    "import pysbd\n",
    "from utils.datautils import avg_top_n\n",
    "import pandas as pd\n",
    "\n",
    "load_user_libs(\"/home/ullriher/lib\", \".path_include\")\n",
    "from alignscore import AlignScore\n",
    "\n",
    "#segmenter \n",
    "segmenter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "sent_tokenize = segmenter.segment\n",
    "\n",
    "align_large = AlignScore(model='roberta-large', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-large.ckpt', evaluation_mode='nli_sp')\n",
    "align_base = AlignScore(model='roberta-base', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "\n",
    "\n",
    "def bertscore_solve(premise, claim):\n",
    "    text_sentencewise = sent_tokenize(premise)\n",
    "    P, R, F = bert_score.score([claim] * len(text_sentencewise), text_sentencewise, model_type=\"roberta-base\")\n",
    "\n",
    "    return (float(avg_top_n(P, 2)),float(avg_top_n(R, 2)),float(avg_top_n(F, 2)))\n",
    "\n",
    "\n",
    "# arse first argument as model name\n",
    "if False:\n",
    "    model_name = sys.argv[1]\n",
    "else:\n",
    "    model_name = \"t5_small_multiclaim\"\n",
    "\n",
    "metric = \"faithfulness\"\n",
    "\n",
    "print(f\"Computing metric: {metric}\")\n",
    "print(f\"Using model {model_name}\")\n",
    "\n",
    "\n",
    "df = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf_multiclaim/test.jsonl\", lines=True)\n",
    "df[\"generated\"] = None\n",
    "predictions = f\"/home/ullriher/ullriher/data/_paper/predictions/{model_name}.jsonl\"\n",
    "outfile = f\"/home/ullriher/ullriher/data/_paper/metrics/{metric}/{model_name}.jsonl\"\n",
    "skip_title_from_context = False\n",
    "\n",
    "with open(predictions, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        df.at[i, \"generated\"] = json.loads(line)\n",
    "        # if context starts with source\\n, remove it\n",
    "        if skip_title_from_context:\n",
    "            if df.at[i, \"sentence_context\"].startswith(df.at[i, \"source\"] + \"\\n\"):\n",
    "                df.at[i, \"sentence_context\"] = df.at[i, \"sentence_context\"][len(df.at[i, \"source\"]) + 1 :]\n",
    "\n",
    "# expand df by generated, one row per element of generated list\n",
    "df = df.explode(\"generated\").reset_index(drop=True)\n",
    "df.drop(columns=[\"source_text\"], inplace=True)\n",
    "# remove leading source\\n from sentence_context\n",
    "df[\"bertscore\"] = None\n",
    "df[\"bertscore_avgtop2\"] = None\n",
    "df[\"alignscore_base\"] = None\n",
    "df[\"alignscore_large\"] = None\n",
    "\n",
    "if path.exists(outfile):\n",
    "    df = pd.read_json(outfile, lines=True)\n",
    "    print(f\"Loaded checkpoint from {outfile}\")\n",
    "\n",
    "# if claims column, drop it\n",
    "if \"claims\" in df.columns:\n",
    "    df.drop(columns=[\"claims\"], inplace=True)\n",
    "\n",
    "print(\"predicting\")\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row[\"alignscore_large\"] is not None:\n",
    "        continue\n",
    "\n",
    "    premise = row[\"sentence_context\"]\n",
    "    claim = row[\"generated\"]\n",
    "    \n",
    "    df.at[index, \"bertscore\"] = tuple(map(float, bert_score.score([premise], [claim], model_type=\"roberta-base\")))\n",
    "    df.at[index, \"bertscore_avgtop2\"] = bertscore_solve(premise, claim)\n",
    "    df.at[index, \"alignscore_base\"] = align_base.score(contexts=[premise], claims=[claim])[0]\n",
    "    df.at[index, \"alignscore_large\"] = align_large.score(contexts=[premise], claims=[claim])[0]\n",
    "    \n",
    "    # break at 100\n",
    "    # save df to outfile\n",
    "    if True or index % 100 == 0:\n",
    "        df.to_json(outfile, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7973551750183105, 0.9318747520446777, 0.859382688999176)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bert_score.score([premise], [claim], model_type=\"roberta-base\")\n",
    "# (tensor([0.7974]), tensor([0.9319]), tensor([0.8594]))\n",
    "#convert to tuple of floats\n",
    "s = tuple(map(float, s))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_token_ids': tensor([    0, 50267, 18289,  1840,   605,  9554,  1437, 50266,  2370,  8453,\n",
       "           1437, 50265,   737,   547,     2])}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s><triplet> Harold Godwinson <subj> English king <obj> position held</s>']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harold Godwinson', 'position held', 'English king')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts = extract_triplets(extracted_preds[0])\n",
    "facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_facts(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s><triplet> R. Kelly <subj> Nas <obj> member of <triplet> Nas <subj> R. Kelly <obj> has part</s>']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    subject, object_, relation = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading Named Entity Recognition Pipeline<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading Named Entity Recognition Pipeline\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 00:45:52,928 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading Relation Extraction Pipeline<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading Relation Extraction Pipeline\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('England', 'per:origin', 'English'),\n",
       " ('English', 'org:country_of_headquarters', 'England'),\n",
       " ('Harold Godwinson', 'per:countries_of_residence', 'England'),\n",
       " ('Harold Godwinson', 'per:origin', 'English'),\n",
       " ('Richard III', 'per:countries_of_residence', 'England'),\n",
       " ('Richard III', 'per:origin', 'English'),\n",
       " ('first', 'per:countries_of_residence', 'England'),\n",
       " ('first', 'per:origin', 'English')}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_facts(\"Richard III of England was the first English king to die in battle since Harold Godwinson.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('The Battle of Bosworth Field',\n",
       "  'org:stateorprovince_of_headquarters',\n",
       "  'Leicestershire')}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_facts(\"The Battle of Bosworth Field took place in Leicestershire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus, covfefe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.0.post1 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-large.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:255: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import bert_score\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from utils.datautils import extract_triplets\n",
    "from transformers import pipeline\n",
    "from utils.ntbutils import load_user_libs\n",
    "import pysbd\n",
    "from utils.datautils import avg_top_n\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "load_user_libs(\"/home/ullriher/lib\", \".path_include\")\n",
    "from alignscore import AlignScore\n",
    "from factsumm import FactSumm\n",
    "from sentence_transformers import CrossEncoder\n",
    "deberta = CrossEncoder('cross-encoder/nli-deberta-v3-small')\n",
    "\n",
    "#segmenter \n",
    "factsumm = FactSumm()\n",
    "segmenter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "sent_tokenize = segmenter.segment\n",
    "\n",
    "align_large = AlignScore(model='roberta-large', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-large.ckpt', evaluation_mode='nli_sp', verbose=False)\n",
    "align_base = AlignScore(model='roberta-base', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-base.ckpt', evaluation_mode='nli_sp', verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claims</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R. Kelly sold 40 million albums.\\nR. Kelly is a musician.\\nR. Kelly was recognized as one of the best-selling music artists in the United States.</td>\n",
       "      <td>[R. Kelly has been a guest vocalist for Nas., R. Kelly has been a guest vocalist for Sean Combs., R. Kelly has been a guest vocalist for The Notorious B.I.G., R. Kelly has been recognized by the Recording Industry Association of America., R. Kelly has been recognized as one of the best-selling music artists in the United States.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greenpeace is focused on the issues of over-fishing.\\nGreenpeace focuses on multiple environmental issues.\\nGreenpeace is an organization.\\nGreenpeace is focused on the issues of whaling.\\nGreenpeace is focused on the issues of deforestation.</td>\n",
       "      <td>[Greenpeace is a non-governmental environmental organization., Greenpeace is a US ex-pat environmental activists., Greenpeace is a nonprofit organization., Greenpeace is a non-governmental environmental organization.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girls' Generation is a South Korean girl group.\\nGirls' Generation was formed by S.M. Entertainment.\\nGirls' Generation is a music act.\\nGirls' Generation was formed by a South Korean entertainment company.\\nGirls' Generation is also known as SNSD.</td>\n",
       "      <td>[Girls' Generation is a South Korean girl group., Girls' Generation is formed by S.M. Entertainment., Girls' Generation is a South Korean girl group., Girls' Generation is formed by S.M. Entertainment., Girls' Generation is a group.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                     claims  \\\n",
       "0                                                                                                         R. Kelly sold 40 million albums.\\nR. Kelly is a musician.\\nR. Kelly was recognized as one of the best-selling music artists in the United States.   \n",
       "1        Greenpeace is focused on the issues of over-fishing.\\nGreenpeace focuses on multiple environmental issues.\\nGreenpeace is an organization.\\nGreenpeace is focused on the issues of whaling.\\nGreenpeace is focused on the issues of deforestation.   \n",
       "2  Girls' Generation is a South Korean girl group.\\nGirls' Generation was formed by S.M. Entertainment.\\nGirls' Generation is a music act.\\nGirls' Generation was formed by a South Korean entertainment company.\\nGirls' Generation is also known as SNSD.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                     generated  \n",
       "0  [R. Kelly has been a guest vocalist for Nas., R. Kelly has been a guest vocalist for Sean Combs., R. Kelly has been a guest vocalist for The Notorious B.I.G., R. Kelly has been recognized by the Recording Industry Association of America., R. Kelly has been recognized as one of the best-selling music artists in the United States.]  \n",
       "1                                                                                                                    [Greenpeace is a non-governmental environmental organization., Greenpeace is a US ex-pat environmental activists., Greenpeace is a nonprofit organization., Greenpeace is a non-governmental environmental organization.]  \n",
       "2                                                                                                    [Girls' Generation is a South Korean girl group., Girls' Generation is formed by S.M. Entertainment., Girls' Generation is a South Korean girl group., Girls' Generation is formed by S.M. Entertainment., Girls' Generation is a group.]  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"claims\",\"generated\"]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.00022816141, 0.00032590298, 0.0006033575, 0.00035612102, 0.9962369],\n",
       " 0.19955009)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def focus(gold_claims, predicted_claims, same=False):\n",
    "    if not isinstance(gold_claims, list):\n",
    "        gold_claims = gold_claims.split(\"\\n\")\n",
    "    if not isinstance(predicted_claims, list):\n",
    "        predicted_claims = predicted_claims.split(\"\\n\")\n",
    "    \n",
    "    result = []\n",
    "    for claim in predicted_claims:\n",
    "        gold_claims_copy = gold_claims.copy()\n",
    "        if same: # pop claim from gold_pairs\n",
    "            # copy gold claims\n",
    "            for j in range(len(gold_claims_copy)):\n",
    "                if claim == gold_claims_copy[j]:\n",
    "                    gold_claims_copy.pop(j)\n",
    "                    break\n",
    "        scores=deberta.predict(list(zip(gold_claims_copy, [claim]*len(gold_claims_copy))), apply_softmax=True, show_progress_bar=False)[:,1]\n",
    "        result.append(np.max(scores))\n",
    "    return result, np.mean(result) \n",
    "\n",
    "focus(df.iloc[0][\"claims\"], df.iloc[0][\"generated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0008317941683344543,\n",
       "  0.0013024344807490706,\n",
       "  0.0024810044560581446,\n",
       "  0.050148364156484604,\n",
       "  0.9872347712516785],\n",
       " 0.20839967370266094)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def focus_alignscore(gold_claims, predicted_claims, align=align_base, same=False):\n",
    "    if not isinstance(gold_claims, list):\n",
    "        gold_claims = gold_claims.split(\"\\n\")\n",
    "    if not isinstance(predicted_claims, list):\n",
    "        predicted_claims = predicted_claims.split(\"\\n\")\n",
    "    align.verbose = False\n",
    "    result = []\n",
    "    for claim in predicted_claims:\n",
    "        gold_claims_copy = gold_claims.copy()\n",
    "        if same: # pop claim from gold_pairs\n",
    "            # copy gold claims\n",
    "            for j in range(len(gold_claims_copy)):\n",
    "                if claim == gold_claims_copy[j]:\n",
    "                    gold_claims_copy.pop(j)\n",
    "                    break\n",
    "        score=align.score([\"\\n\".join(gold_claims_copy)], [claim])[0]\n",
    "        result.append(score)\n",
    "    return result, np.mean(result) \n",
    "\n",
    "focus_alignscore(df.iloc[0][\"claims\"], df.iloc[0][\"generated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: multiclaim\n",
      "Using model t5_small_multiclaim\n",
      "Loaded checkpoint from /home/ullriher/ullriher/data/_paper/metrics/multiclaim/t5_small_multiclaim.jsonl\n",
      "predicting\n"
     ]
    }
   ],
   "source": [
    "def bertscore_solve(premise, claim):\n",
    "    text_sentencewise = sent_tokenize(premise)\n",
    "    P, R, F = bert_score.score([claim] * len(text_sentencewise), text_sentencewise, model_type=\"roberta-base\")\n",
    "\n",
    "    return (float(avg_top_n(P, 2)), float(avg_top_n(R, 2)), float(avg_top_n(F, 2)))\n",
    "\n",
    "\n",
    "# arse first argument as model name\n",
    "if False:\n",
    "    model_name = sys.argv[1]\n",
    "else:\n",
    "    model_name = \"t5_small_multiclaim\"\n",
    "\n",
    "metric = \"multiclaim\"\n",
    "\n",
    "print(f\"Computing metric: {metric}\")\n",
    "print(f\"Using model {model_name}\")\n",
    "\n",
    "\n",
    "df = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf_multiclaim/test.jsonl\", lines=True)\n",
    "df[\"generated\"] = None\n",
    "predictions = f\"/home/ullriher/ullriher/data/_paper/predictions/{model_name}.jsonl\"\n",
    "outfile = f\"/home/ullriher/ullriher/data/_paper/metrics/{metric}/{model_name}.jsonl\"\n",
    "skip_title_from_context = False\n",
    "\n",
    "with open(predictions, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        df.at[i, \"generated\"] = json.loads(line)\n",
    "        # if context starts with source\\n, remove it\n",
    "        if skip_title_from_context:\n",
    "            if df.at[i, \"sentence_context\"].startswith(df.at[i, \"source\"] + \"\\n\"):\n",
    "                df.at[i, \"sentence_context\"] = df.at[i, \"sentence_context\"][len(df.at[i, \"source\"]) + 1 :]\n",
    "\n",
    "# expand df by generated, one row per element of generated list\n",
    "# df = df.explode(\"generated\").reset_index(drop=True)\n",
    "df.drop(columns=[\"source_text\"], inplace=True)\n",
    "# remove leading source\\n from sentence_context\n",
    "df[\"claims\"] = df[\"claims\"].str.split(\"\\n\")\n",
    "\n",
    "df[\"f_deberta\"]=None\n",
    "df[\"c_deberta\"]=None\n",
    "df[\"r_deberta\"]=None\n",
    "df[\"f_align\"]=None\n",
    "df[\"c_align\"]=None\n",
    "df[\"r_align\"]=None\n",
    "df[\"f_align_mean\"]=None\n",
    "df[\"f_deberta_mean\"]=None\n",
    "df[\"c_deberta_mean\"]=None\n",
    "df[\"c_align_mean\"]=None\n",
    "df[\"r_deberta_mean\"]=None\n",
    "df[\"r_align_mean\"]=None\n",
    "\n",
    "if path.exists(outfile):\n",
    "    df = pd.read_json(outfile, lines=True)\n",
    "    print(f\"Loaded checkpoint from {outfile}\")\n",
    "\n",
    "df = df.dropna(subset=[\"generated\"])\n",
    "print(\"predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [00:45,  6.07it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# break at 100\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# save df to outfile\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/pandas/core/generic.py:2650\u001b[0m, in \u001b[0;36mNDFrame.to_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options)\u001b[0m\n\u001b[1;32m   2647\u001b[0m config\u001b[38;5;241m.\u001b[39mis_nonnegative_int(indent)\n\u001b[1;32m   2648\u001b[0m indent \u001b[38;5;241m=\u001b[39m indent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 2650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/pandas/io/json/_json.py:178\u001b[0m, in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent, storage_options)\u001b[0m\n\u001b[1;32m    174\u001b[0m     s \u001b[38;5;241m=\u001b[39m convert_to_line_delimits(s)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path_or_buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    181\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mwrite(s)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/2023feb/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[0;32m/mnt/appl/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/codecs.py:186\u001b[0m, in \u001b[0;36mIncrementalEncoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalEncoder\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    An IncrementalEncoder encodes an input in multiple steps. The input can\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    be passed piece by piece to the encode() method. The IncrementalEncoder\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    remembers the state of the encoding process between calls to encode().\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        Creates an IncrementalEncoder instance.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m        for a list of possible values.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m errors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row[\"r_align\"] is not None:\n",
    "        continue\n",
    "    claims = row[\"generated\"]\n",
    "    # if claims or generated is empty, skip\n",
    "    df.at[index, \"f_deberta\"], df.at[index, \"f_deberta_mean\"] = focus(row[\"claims\"], claims)\n",
    "    df.at[index, \"c_deberta\"], df.at[index, \"c_deberta_mean\"] = focus(claims, row[\"claims\"])\n",
    "    df.at[index, \"r_deberta\"], df.at[index, \"r_deberta_mean\"] = focus(claims, claims, same=True)\n",
    "    df.at[index, \"f_align\"], df.at[index, \"f_align_mean\"] = focus_alignscore(row[\"claims\"], claims)\n",
    "    df.at[index, \"c_align\"], df.at[index, \"c_align_mean\"] = focus_alignscore(claims, row[\"claims\"])\n",
    "    df.at[index, \"r_align\"], df.at[index, \"r_align_mean\"] = focus_alignscore(claims, claims, same=True)\n",
    "    \n",
    "    # break at 100\n",
    "    # save df to outfile\n",
    "    if True or index % 100 == 0:\n",
    "        df.to_json(outfile, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v1.9.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file models/alignscore/AlignScore-base.ckpt`\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ullriher/venvs/2023feb/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:255: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: multiclaim\n",
      "Using model t5_small_multiclaim\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from utils.datautils import extract_triplets\n",
    "from transformers import pipeline\n",
    "from utils.ntbutils import load_user_libs\n",
    "import pysbd\n",
    "from utils.datautils import avg_top_n\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "load_user_libs(\"/home/ullriher/lib\", \".path_include\")\n",
    "from alignscore import AlignScore\n",
    "from factsumm import FactSumm\n",
    "from sentence_transformers import CrossEncoder\n",
    "deberta = CrossEncoder('cross-encoder/nli-deberta-v3-small')\n",
    "\n",
    "#segmenter \n",
    "#factsumm = FactSumm()\n",
    "segmenter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "sent_tokenize = segmenter.segment\n",
    "\n",
    "# align_large = AlignScore(model='roberta-large', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-large.ckpt', evaluation_mode='nli_sp', verbose=False)\n",
    "align_base = AlignScore(model='roberta-base', batch_size=32, device=\"cuda:0\", ckpt_path='/home/ullriher/ullriher/models/alignscore/AlignScore-base.ckpt', evaluation_mode='nli_sp', verbose=False)\n",
    "\n",
    "\n",
    "def focus(gold_claims, predicted_claims, same=False):\n",
    "    if not isinstance(gold_claims, list):\n",
    "        gold_claims = gold_claims.split(\"\\n\")\n",
    "    if not isinstance(predicted_claims, list):\n",
    "        predicted_claims = predicted_claims.split(\"\\n\")\n",
    "    if len(gold_claims) == 0 or len(predicted_claims) == 0:\n",
    "        return [], 0\n",
    "    \n",
    "    result = []\n",
    "    for claim in predicted_claims:\n",
    "        gold_claims_copy = gold_claims.copy()\n",
    "        if same: # pop claim from gold_pairs\n",
    "            # copy gold claims\n",
    "            for j in range(len(gold_claims_copy)):\n",
    "                if claim == gold_claims_copy[j]:\n",
    "                    gold_claims_copy.pop(j)\n",
    "                    break\n",
    "        scores=deberta.predict(list(zip(gold_claims_copy, [claim]*len(gold_claims_copy))), apply_softmax=True, show_progress_bar=False)[:,1]\n",
    "        result.append(np.max(scores))\n",
    "    return result, np.mean(result) \n",
    "\n",
    "def focus_alignscore(gold_claims, predicted_claims, align=align_base, same=False):\n",
    "    if not isinstance(gold_claims, list):\n",
    "        gold_claims = gold_claims.split(\"\\n\")\n",
    "    if not isinstance(predicted_claims, list):\n",
    "        predicted_claims = predicted_claims.split(\"\\n\")\n",
    "    if len(gold_claims) == 0 or len(predicted_claims) == 0:\n",
    "        return [], 0\n",
    "    align.verbose = False\n",
    "    result = []\n",
    "    for claim in predicted_claims:\n",
    "        gold_claims_copy = gold_claims.copy()\n",
    "        if same: # pop claim from gold_pairs\n",
    "            # copy gold claims\n",
    "            for j in range(len(gold_claims_copy)):\n",
    "                if claim == gold_claims_copy[j]:\n",
    "                    gold_claims_copy.pop(j)\n",
    "                    break\n",
    "        score=align.score([\"\\n\".join(gold_claims_copy)], [claim])[0]\n",
    "        result.append(score)\n",
    "    return result, np.mean(result) \n",
    "\n",
    "\n",
    "# arse first argument as model name\n",
    "if False:\n",
    "    model_name = sys.argv[1]\n",
    "else:\n",
    "    model_name = \"t5_small_multiclaim\"\n",
    "\n",
    "metric = \"multiclaim\"\n",
    "\n",
    "print(f\"Computing metric: {metric}\")\n",
    "print(f\"Using model {model_name}\")\n",
    "\n",
    "\n",
    "df = pd.read_json(\"/mnt/data/factcheck/claim_extraction/feversum/hf_multiclaim/test.jsonl\", lines=True)\n",
    "df[\"generated\"] = None\n",
    "predictions = f\"/home/ullriher/ullriher/data/_paper/predictions/{model_name}.jsonl\"\n",
    "outfile = f\"/home/ullriher/ullriher/data/_paper/metrics/{metric}/{model_name}.jsonl\"\n",
    "skip_title_from_context = False\n",
    "\n",
    "with open(predictions, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        df.at[i, \"generated\"] = json.loads(line)\n",
    "        # if context starts with source\\n, remove it\n",
    "        if skip_title_from_context:\n",
    "            if df.at[i, \"sentence_context\"].startswith(df.at[i, \"source\"] + \"\\n\"):\n",
    "                df.at[i, \"sentence_context\"] = df.at[i, \"sentence_context\"][len(df.at[i, \"source\"]) + 1 :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>claims</th>\n",
       "      <th>source_text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_context</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>R. Kelly sold 40 million albums.\\nR. Kelly is ...</td>\n",
       "      <td>R. Kelly\\nRobert Sylvester Kelly (born January...</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration...</td>\n",
       "      <td>R. Kelly\\nIn 1996, Kelly was nominated for a G...</td>\n",
       "      <td>[R. Kelly has been a guest vocalist for Nas., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>4799</td>\n",
       "      <td>Greenpeace is focused on the issues of over-fi...</td>\n",
       "      <td>Greenpeace\\nGreenpeace is a non-governmental e...</td>\n",
       "      <td>Founded by Canadian and US ex-pat environmenta...</td>\n",
       "      <td>Greenpeace\\nGreenpeace is a non-governmental e...</td>\n",
       "      <td>[Greenpeace is a non-governmental environmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girls' Generation</td>\n",
       "      <td>4773</td>\n",
       "      <td>Girls' Generation is a South Korean girl group...</td>\n",
       "      <td>Girls' Generation\\nGirls' Generation, also kno...</td>\n",
       "      <td>Girls' Generation, also known as SNSD, is a So...</td>\n",
       "      <td>Girls' Generation\\nGirls' Generation, also kno...</td>\n",
       "      <td>[Girls' Generation is a South Korean girl grou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source  sentence_id  \\\n",
       "0           R. Kelly         6140   \n",
       "1         Greenpeace         4799   \n",
       "2  Girls' Generation         4773   \n",
       "\n",
       "                                              claims  \\\n",
       "0  R. Kelly sold 40 million albums.\\nR. Kelly is ...   \n",
       "1  Greenpeace is focused on the issues of over-fi...   \n",
       "2  Girls' Generation is a South Korean girl group...   \n",
       "\n",
       "                                         source_text  \\\n",
       "0  R. Kelly\\nRobert Sylvester Kelly (born January...   \n",
       "1  Greenpeace\\nGreenpeace is a non-governmental e...   \n",
       "2  Girls' Generation\\nGirls' Generation, also kno...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  In 2002 and 2004, Kelly released collaboration...   \n",
       "1  Founded by Canadian and US ex-pat environmenta...   \n",
       "2  Girls' Generation, also known as SNSD, is a So...   \n",
       "\n",
       "                                    sentence_context  \\\n",
       "0  R. Kelly\\nIn 1996, Kelly was nominated for a G...   \n",
       "1  Greenpeace\\nGreenpeace is a non-governmental e...   \n",
       "2  Girls' Generation\\nGirls' Generation, also kno...   \n",
       "\n",
       "                                           generated  \n",
       "0  [R. Kelly has been a guest vocalist for Nas., ...  \n",
       "1  [Greenpeace is a non-governmental environmenta...  \n",
       "2  [Girls' Generation is a South Korean girl grou...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>claims</th>\n",
       "      <th>source_text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_context</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [source, sentence_id, claims, source_text, sentence, sentence_context, generated]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show where generated is null\n",
    "df[df[\"generated\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from /home/ullriher/ullriher/data/_paper/metrics/multiclaim/t5_small_multiclaim.jsonl\n",
      "predicting\n"
     ]
    }
   ],
   "source": [
    "# expand df by generated, one row per element of generated list\n",
    "# df = df.explode(\"generated\").reset_index(drop=True)\n",
    "if \"source_text\" in df.columns: \n",
    "    df.drop(columns=[\"source_text\"], inplace=True)\n",
    "# remove leading source\\n from sentence_context\n",
    "df[\"claims\"] = df[\"claims\"].str.split(\"\\n\")\n",
    "\n",
    "df[\"f_deberta\"]=None\n",
    "df[\"c_deberta\"]=None\n",
    "df[\"r_deberta\"]=None\n",
    "df[\"f_align\"]=None\n",
    "df[\"c_align\"]=None\n",
    "df[\"r_align\"]=None\n",
    "df[\"f_align_mean\"]=None\n",
    "df[\"f_deberta_mean\"]=None\n",
    "df[\"c_deberta_mean\"]=None\n",
    "df[\"c_align_mean\"]=None\n",
    "df[\"r_deberta_mean\"]=None\n",
    "df[\"r_align_mean\"]=None\n",
    "\n",
    "if path.exists(outfile):\n",
    "    df = pd.read_json(outfile, lines=True)\n",
    "    print(f\"Loaded checkpoint from {outfile}\")\n",
    "\n",
    "df = df.dropna(subset=[\"generated\"])\n",
    "print(\"predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>claims</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_context</th>\n",
       "      <th>generated</th>\n",
       "      <th>f_deberta</th>\n",
       "      <th>c_deberta</th>\n",
       "      <th>r_deberta</th>\n",
       "      <th>f_align</th>\n",
       "      <th>c_align</th>\n",
       "      <th>r_align</th>\n",
       "      <th>f_align_mean</th>\n",
       "      <th>f_deberta_mean</th>\n",
       "      <th>c_deberta_mean</th>\n",
       "      <th>c_align_mean</th>\n",
       "      <th>r_deberta_mean</th>\n",
       "      <th>r_align_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R. Kelly</td>\n",
       "      <td>6140</td>\n",
       "      <td>[R. Kelly sold 40 million albums., R. Kelly is...</td>\n",
       "      <td>In 2002 and 2004, Kelly released collaboration...</td>\n",
       "      <td>R. Kelly\\nIn 1996, Kelly was nominated for a G...</td>\n",
       "      <td>[R. Kelly has been a guest vocalist for Nas., ...</td>\n",
       "      <td>[0.0002281614, 0.000325903, 0.0006033575000000...</td>\n",
       "      <td>[0.0001285921, 0.9811982512, 0.995229125]</td>\n",
       "      <td>[0.00027731990000000003, 0.0001378678000000000...</td>\n",
       "      <td>[0.0008317942, 0.0013024345, 0.0024810045, 0.0...</td>\n",
       "      <td>[0.0022423693000000002, 0.9961805344, 0.987279...</td>\n",
       "      <td>[0.0048222994000000005, 0.0043610968, 0.010591...</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.199550</td>\n",
       "      <td>0.658852</td>\n",
       "      <td>0.661901</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.022772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greenpeace</td>\n",
       "      <td>4799</td>\n",
       "      <td>[Greenpeace is focused on the issues of over-f...</td>\n",
       "      <td>Founded by Canadian and US ex-pat environmenta...</td>\n",
       "      <td>Greenpeace\\nGreenpeace is a non-governmental e...</td>\n",
       "      <td>[Greenpeace is a non-governmental environmenta...</td>\n",
       "      <td>[0.001277686, 0.0001793168, 0.0019212369, 0.00...</td>\n",
       "      <td>[0.00021571100000000002, 0.10045540330000001, ...</td>\n",
       "      <td>[0.9955329895, 9.41527e-05, 0.9884605408, 0.99...</td>\n",
       "      <td>[0.0733137056, 0.0035160400000000002, 0.022341...</td>\n",
       "      <td>[0.0006854869, 0.1337228417, 0.9841062427, 0.0...</td>\n",
       "      <td>[0.9759155512000001, 0.0031969612, 0.133909627...</td>\n",
       "      <td>0.043121</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.219137</td>\n",
       "      <td>0.223968</td>\n",
       "      <td>0.744905</td>\n",
       "      <td>0.522234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Girls' Generation</td>\n",
       "      <td>4773</td>\n",
       "      <td>[Girls' Generation is a South Korean girl grou...</td>\n",
       "      <td>Girls' Generation, also known as SNSD, is a So...</td>\n",
       "      <td>Girls' Generation\\nGirls' Generation, also kno...</td>\n",
       "      <td>[Girls' Generation is a South Korean girl grou...</td>\n",
       "      <td>[0.9857805371, 0.9966781139, 0.9857805371, 0.9...</td>\n",
       "      <td>[0.9857805371, 0.9961774349, 0.007004799300000...</td>\n",
       "      <td>[0.9857805371, 0.9911391139000001, 0.985780537...</td>\n",
       "      <td>[0.9809049368, 0.9545260668000001, 0.980904936...</td>\n",
       "      <td>[0.9862440228, 0.9725143909, 0.6093394756, 0.9...</td>\n",
       "      <td>[0.9858044982, 0.9702588320000001, 0.985804498...</td>\n",
       "      <td>0.971409</td>\n",
       "      <td>0.990029</td>\n",
       "      <td>0.593890</td>\n",
       "      <td>0.704504</td>\n",
       "      <td>0.987814</td>\n",
       "      <td>0.981001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Lopes</td>\n",
       "      <td>617</td>\n",
       "      <td>[Lisa Lopes collaborated with rapper Lil' Kim....</td>\n",
       "      <td>Lisa Nicole Lopes (May 27, 1971 - April 25, 20...</td>\n",
       "      <td>Lisa Lopes\\nLisa Nicole Lopes (May 27, 1971 - ...</td>\n",
       "      <td>[Lisa Lopes was a hip hop singer., Lisa Lopes ...</td>\n",
       "      <td>[0.3414902091, 0.9561726451, 0.0172328167, 0.0...</td>\n",
       "      <td>[0.00041509500000000004, 0.0137727642, 0.01636...</td>\n",
       "      <td>[0.9940065145, 0.7346350551, 0.0008511407, 0.1...</td>\n",
       "      <td>[0.0144883571, 0.0024146722, 0.0859003961, 0.0...</td>\n",
       "      <td>[0.0033907159, 0.0025693241, 0.0064581046, 0.0...</td>\n",
       "      <td>[0.938795507, 0.0538470112, 0.0028041604, 0.01...</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>0.279670</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.473433</td>\n",
       "      <td>0.326412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linkin Park</td>\n",
       "      <td>5387</td>\n",
       "      <td>[Linkin Park released the album Hybrid Theory....</td>\n",
       "      <td>Formed in 1996, the band rose to international...</td>\n",
       "      <td>Linkin Park\\nLinkin Park is an American rock b...</td>\n",
       "      <td>[Linkin Park is a rock band., Linkin Park's de...</td>\n",
       "      <td>[0.8129991889, 0.0013062379000000001, 0.000167...</td>\n",
       "      <td>[0.9977391958, 0.9891306162000001]</td>\n",
       "      <td>[0.8427196145, 0.00021641180000000001, 0.00015...</td>\n",
       "      <td>[0.0634528026, 0.0574352629, 0.002694009100000...</td>\n",
       "      <td>[0.9855245948, 0.9902856946]</td>\n",
       "      <td>[0.3731473386, 0.0019890338, 0.000768648100000...</td>\n",
       "      <td>0.032827</td>\n",
       "      <td>0.203682</td>\n",
       "      <td>0.993435</td>\n",
       "      <td>0.987905</td>\n",
       "      <td>0.210814</td>\n",
       "      <td>0.095506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Angela Lansbury</td>\n",
       "      <td>6003</td>\n",
       "      <td>[Angela Lansbury contributed to animated films...</td>\n",
       "      <td>She also moved into voice work, thereby contri...</td>\n",
       "      <td>Angela Lansbury\\nThrough Corymore Productions,...</td>\n",
       "      <td>[Angela Lansbury contributed to Disney's Beaut...</td>\n",
       "      <td>[0.000523449, 0.9968160987, 0.0004051966]</td>\n",
       "      <td>[0.9968160987, 0.9545787573000001]</td>\n",
       "      <td>[0.058683347000000004, 0.9861628413, 0.9956840...</td>\n",
       "      <td>[0.0018639604000000001, 0.9938004613, 0.001941...</td>\n",
       "      <td>[0.9853401780000001, 0.06975777450000001]</td>\n",
       "      <td>[0.47333866360000004, 0.9294198155000001, 0.98...</td>\n",
       "      <td>0.332535</td>\n",
       "      <td>0.332582</td>\n",
       "      <td>0.975697</td>\n",
       "      <td>0.527549</td>\n",
       "      <td>0.680177</td>\n",
       "      <td>0.796507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Seinfeld</td>\n",
       "      <td>4679</td>\n",
       "      <td>[Seinfeld was named the \"number 1 reason the '...</td>\n",
       "      <td>E! named the series the \"number 1 reason the'9...</td>\n",
       "      <td>Seinfeld\\nIn 2013, the Writers Guild of Americ...</td>\n",
       "      <td>[Seinfeld is a TV series., Seinfeld is a serie...</td>\n",
       "      <td>[0.004850402, 0.0220304672, 0.0220304672]</td>\n",
       "      <td>[0.0001621146, 0.0006575976]</td>\n",
       "      <td>[0.9433251023, 0.9953304529, 0.9953304529]</td>\n",
       "      <td>[0.1863470674, 0.2847853005, 0.2847853005]</td>\n",
       "      <td>[0.0048092376, 0.0224094186]</td>\n",
       "      <td>[0.9391465187, 0.9925837517, 0.9925837517]</td>\n",
       "      <td>0.251973</td>\n",
       "      <td>0.016304</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>0.977995</td>\n",
       "      <td>0.974771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Sherlock (TV series)</td>\n",
       "      <td>5302</td>\n",
       "      <td>[Sherlock has thirteen episodes., Sherlock is ...</td>\n",
       "      <td>Thirteen episodes have been produced, with thr...</td>\n",
       "      <td>Sherlock (TV series)\\nCreated by Steven Moffat...</td>\n",
       "      <td>[Sherlock is a television series., Sherlock is...</td>\n",
       "      <td>[0.9980496168, 0.9957154393000001, 0.000801315...</td>\n",
       "      <td>[0.1388856322, 0.9979077578000001]</td>\n",
       "      <td>[0.9918445945000001, 0.9954407215000001, 0.000...</td>\n",
       "      <td>[0.9894272685000001, 0.9855688214, 0.004765541...</td>\n",
       "      <td>[0.0025653131, 0.9859529734]</td>\n",
       "      <td>[0.9809195399, 0.9895373583, 0.004161926000000...</td>\n",
       "      <td>0.741333</td>\n",
       "      <td>0.747570</td>\n",
       "      <td>0.568397</td>\n",
       "      <td>0.494259</td>\n",
       "      <td>0.745783</td>\n",
       "      <td>0.741039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Christopher Lloyd</td>\n",
       "      <td>4911</td>\n",
       "      <td>[Christopher Lloyd voiced a character name The...</td>\n",
       "      <td>He has also done extensive voiceover work for ...</td>\n",
       "      <td>Christopher Lloyd\\nHe earned a third Emmy for ...</td>\n",
       "      <td>[Christopher Lloyd voicing The Hacker., Christ...</td>\n",
       "      <td>[0.9894266129, 0.9683456421000001, 0.982550323...</td>\n",
       "      <td>[0.9775460362, 0.9943634272, 0.002574031, 0.98...</td>\n",
       "      <td>[0.00020294560000000002, 0.9619967937, 0.00039...</td>\n",
       "      <td>[0.9046993256, 0.9577310085, 0.43554458020000003]</td>\n",
       "      <td>[0.9536542892000001, 0.9738487005, 0.370740354...</td>\n",
       "      <td>[0.0044621211, 0.28494578600000003, 0.00279645...</td>\n",
       "      <td>0.765992</td>\n",
       "      <td>0.980108</td>\n",
       "      <td>0.566172</td>\n",
       "      <td>0.605810</td>\n",
       "      <td>0.320866</td>\n",
       "      <td>0.097401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Adrianne Palicki</td>\n",
       "      <td>3541</td>\n",
       "      <td>[Adrianne Palicki acted in Friday Night Lights...</td>\n",
       "      <td>Adrianne Lee Palicki (born May 6, 1983) is an ...</td>\n",
       "      <td>Adrianne Palicki\\nAdrianne Lee Palicki (born M...</td>\n",
       "      <td>[Adrianne Palicki is an actress., Adrianne Pal...</td>\n",
       "      <td>[0.9949715137, 0.9943859577, 0.9921808243, 0.0...</td>\n",
       "      <td>[0.0007374774, 0.9943859577, 0.000117187700000...</td>\n",
       "      <td>[0.9518859386, 0.0001180501, 0.0016440597, 0.0...</td>\n",
       "      <td>[0.9487306476, 0.978271544, 0.9525666833, 0.00...</td>\n",
       "      <td>[0.0040386212, 0.9870303273000001, 0.004068435...</td>\n",
       "      <td>[0.7562144995000001, 0.0006456591000000001, 0....</td>\n",
       "      <td>0.480740</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.259560</td>\n",
       "      <td>0.491369</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.126561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   source  sentence_id  \\\n",
       "0                R. Kelly         6140   \n",
       "1              Greenpeace         4799   \n",
       "2       Girls' Generation         4773   \n",
       "3              Lisa Lopes          617   \n",
       "4             Linkin Park         5387   \n",
       "..                    ...          ...   \n",
       "439       Angela Lansbury         6003   \n",
       "440              Seinfeld         4679   \n",
       "441  Sherlock (TV series)         5302   \n",
       "442     Christopher Lloyd         4911   \n",
       "443      Adrianne Palicki         3541   \n",
       "\n",
       "                                                claims  \\\n",
       "0    [R. Kelly sold 40 million albums., R. Kelly is...   \n",
       "1    [Greenpeace is focused on the issues of over-f...   \n",
       "2    [Girls' Generation is a South Korean girl grou...   \n",
       "3    [Lisa Lopes collaborated with rapper Lil' Kim....   \n",
       "4    [Linkin Park released the album Hybrid Theory....   \n",
       "..                                                 ...   \n",
       "439  [Angela Lansbury contributed to animated films...   \n",
       "440  [Seinfeld was named the \"number 1 reason the '...   \n",
       "441  [Sherlock has thirteen episodes., Sherlock is ...   \n",
       "442  [Christopher Lloyd voiced a character name The...   \n",
       "443  [Adrianne Palicki acted in Friday Night Lights...   \n",
       "\n",
       "                                              sentence  \\\n",
       "0    In 2002 and 2004, Kelly released collaboration...   \n",
       "1    Founded by Canadian and US ex-pat environmenta...   \n",
       "2    Girls' Generation, also known as SNSD, is a So...   \n",
       "3    Lisa Nicole Lopes (May 27, 1971 - April 25, 20...   \n",
       "4    Formed in 1996, the band rose to international...   \n",
       "..                                                 ...   \n",
       "439  She also moved into voice work, thereby contri...   \n",
       "440  E! named the series the \"number 1 reason the'9...   \n",
       "441  Thirteen episodes have been produced, with thr...   \n",
       "442  He has also done extensive voiceover work for ...   \n",
       "443  Adrianne Lee Palicki (born May 6, 1983) is an ...   \n",
       "\n",
       "                                      sentence_context  \\\n",
       "0    R. Kelly\\nIn 1996, Kelly was nominated for a G...   \n",
       "1    Greenpeace\\nGreenpeace is a non-governmental e...   \n",
       "2    Girls' Generation\\nGirls' Generation, also kno...   \n",
       "3    Lisa Lopes\\nLisa Nicole Lopes (May 27, 1971 - ...   \n",
       "4    Linkin Park\\nLinkin Park is an American rock b...   \n",
       "..                                                 ...   \n",
       "439  Angela Lansbury\\nThrough Corymore Productions,...   \n",
       "440  Seinfeld\\nIn 2013, the Writers Guild of Americ...   \n",
       "441  Sherlock (TV series)\\nCreated by Steven Moffat...   \n",
       "442  Christopher Lloyd\\nHe earned a third Emmy for ...   \n",
       "443  Adrianne Palicki\\nAdrianne Lee Palicki (born M...   \n",
       "\n",
       "                                             generated  \\\n",
       "0    [R. Kelly has been a guest vocalist for Nas., ...   \n",
       "1    [Greenpeace is a non-governmental environmenta...   \n",
       "2    [Girls' Generation is a South Korean girl grou...   \n",
       "3    [Lisa Lopes was a hip hop singer., Lisa Lopes ...   \n",
       "4    [Linkin Park is a rock band., Linkin Park's de...   \n",
       "..                                                 ...   \n",
       "439  [Angela Lansbury contributed to Disney's Beaut...   \n",
       "440  [Seinfeld is a TV series., Seinfeld is a serie...   \n",
       "441  [Sherlock is a television series., Sherlock is...   \n",
       "442  [Christopher Lloyd voicing The Hacker., Christ...   \n",
       "443  [Adrianne Palicki is an actress., Adrianne Pal...   \n",
       "\n",
       "                                             f_deberta  \\\n",
       "0    [0.0002281614, 0.000325903, 0.0006033575000000...   \n",
       "1    [0.001277686, 0.0001793168, 0.0019212369, 0.00...   \n",
       "2    [0.9857805371, 0.9966781139, 0.9857805371, 0.9...   \n",
       "3    [0.3414902091, 0.9561726451, 0.0172328167, 0.0...   \n",
       "4    [0.8129991889, 0.0013062379000000001, 0.000167...   \n",
       "..                                                 ...   \n",
       "439          [0.000523449, 0.9968160987, 0.0004051966]   \n",
       "440          [0.004850402, 0.0220304672, 0.0220304672]   \n",
       "441  [0.9980496168, 0.9957154393000001, 0.000801315...   \n",
       "442  [0.9894266129, 0.9683456421000001, 0.982550323...   \n",
       "443  [0.9949715137, 0.9943859577, 0.9921808243, 0.0...   \n",
       "\n",
       "                                             c_deberta  \\\n",
       "0            [0.0001285921, 0.9811982512, 0.995229125]   \n",
       "1    [0.00021571100000000002, 0.10045540330000001, ...   \n",
       "2    [0.9857805371, 0.9961774349, 0.007004799300000...   \n",
       "3    [0.00041509500000000004, 0.0137727642, 0.01636...   \n",
       "4                   [0.9977391958, 0.9891306162000001]   \n",
       "..                                                 ...   \n",
       "439                 [0.9968160987, 0.9545787573000001]   \n",
       "440                       [0.0001621146, 0.0006575976]   \n",
       "441                 [0.1388856322, 0.9979077578000001]   \n",
       "442  [0.9775460362, 0.9943634272, 0.002574031, 0.98...   \n",
       "443  [0.0007374774, 0.9943859577, 0.000117187700000...   \n",
       "\n",
       "                                             r_deberta  \\\n",
       "0    [0.00027731990000000003, 0.0001378678000000000...   \n",
       "1    [0.9955329895, 9.41527e-05, 0.9884605408, 0.99...   \n",
       "2    [0.9857805371, 0.9911391139000001, 0.985780537...   \n",
       "3    [0.9940065145, 0.7346350551, 0.0008511407, 0.1...   \n",
       "4    [0.8427196145, 0.00021641180000000001, 0.00015...   \n",
       "..                                                 ...   \n",
       "439  [0.058683347000000004, 0.9861628413, 0.9956840...   \n",
       "440         [0.9433251023, 0.9953304529, 0.9953304529]   \n",
       "441  [0.9918445945000001, 0.9954407215000001, 0.000...   \n",
       "442  [0.00020294560000000002, 0.9619967937, 0.00039...   \n",
       "443  [0.9518859386, 0.0001180501, 0.0016440597, 0.0...   \n",
       "\n",
       "                                               f_align  \\\n",
       "0    [0.0008317942, 0.0013024345, 0.0024810045, 0.0...   \n",
       "1    [0.0733137056, 0.0035160400000000002, 0.022341...   \n",
       "2    [0.9809049368, 0.9545260668000001, 0.980904936...   \n",
       "3    [0.0144883571, 0.0024146722, 0.0859003961, 0.0...   \n",
       "4    [0.0634528026, 0.0574352629, 0.002694009100000...   \n",
       "..                                                 ...   \n",
       "439  [0.0018639604000000001, 0.9938004613, 0.001941...   \n",
       "440         [0.1863470674, 0.2847853005, 0.2847853005]   \n",
       "441  [0.9894272685000001, 0.9855688214, 0.004765541...   \n",
       "442  [0.9046993256, 0.9577310085, 0.43554458020000003]   \n",
       "443  [0.9487306476, 0.978271544, 0.9525666833, 0.00...   \n",
       "\n",
       "                                               c_align  \\\n",
       "0    [0.0022423693000000002, 0.9961805344, 0.987279...   \n",
       "1    [0.0006854869, 0.1337228417, 0.9841062427, 0.0...   \n",
       "2    [0.9862440228, 0.9725143909, 0.6093394756, 0.9...   \n",
       "3    [0.0033907159, 0.0025693241, 0.0064581046, 0.0...   \n",
       "4                         [0.9855245948, 0.9902856946]   \n",
       "..                                                 ...   \n",
       "439          [0.9853401780000001, 0.06975777450000001]   \n",
       "440                       [0.0048092376, 0.0224094186]   \n",
       "441                       [0.0025653131, 0.9859529734]   \n",
       "442  [0.9536542892000001, 0.9738487005, 0.370740354...   \n",
       "443  [0.0040386212, 0.9870303273000001, 0.004068435...   \n",
       "\n",
       "                                               r_align  f_align_mean  \\\n",
       "0    [0.0048222994000000005, 0.0043610968, 0.010591...      0.208400   \n",
       "1    [0.9759155512000001, 0.0031969612, 0.133909627...      0.043121   \n",
       "2    [0.9858044982, 0.9702588320000001, 0.985804498...      0.971409   \n",
       "3    [0.938795507, 0.0538470112, 0.0028041604, 0.01...      0.021860   \n",
       "4    [0.3731473386, 0.0019890338, 0.000768648100000...      0.032827   \n",
       "..                                                 ...           ...   \n",
       "439  [0.47333866360000004, 0.9294198155000001, 0.98...      0.332535   \n",
       "440         [0.9391465187, 0.9925837517, 0.9925837517]      0.251973   \n",
       "441  [0.9809195399, 0.9895373583, 0.004161926000000...      0.741333   \n",
       "442  [0.0044621211, 0.28494578600000003, 0.00279645...      0.765992   \n",
       "443  [0.7562144995000001, 0.0006456591000000001, 0....      0.480740   \n",
       "\n",
       "     f_deberta_mean  c_deberta_mean  c_align_mean  r_deberta_mean  \\\n",
       "0          0.199550        0.658852      0.661901        0.000200   \n",
       "1          0.001164        0.219137      0.223968        0.744905   \n",
       "2          0.990029        0.593890      0.704504        0.987814   \n",
       "3          0.279670        0.006249      0.003524        0.473433   \n",
       "4          0.203682        0.993435      0.987905        0.210814   \n",
       "..              ...             ...           ...             ...   \n",
       "439        0.332582        0.975697      0.527549        0.680177   \n",
       "440        0.016304        0.000410      0.013609        0.977995   \n",
       "441        0.747570        0.568397      0.494259        0.745783   \n",
       "442        0.980108        0.566172      0.605810        0.320866   \n",
       "443        0.497143        0.259560      0.491369        0.159139   \n",
       "\n",
       "     r_align_mean  \n",
       "0        0.022772  \n",
       "1        0.522234  \n",
       "2        0.981001  \n",
       "3        0.326412  \n",
       "4        0.095506  \n",
       "..            ...  \n",
       "439      0.796507  \n",
       "440      0.974771  \n",
       "441      0.741039  \n",
       "442      0.097401  \n",
       "443      0.126561  \n",
       "\n",
       "[444 rows x 18 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "444it [00:00, 23800.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    if row[\"r_align\"] is not None:\n",
    "        continue\n",
    "    claims = list(row[\"generated\"].copy())\n",
    "    gold_claims = list(row[\"claims\"].copy())\n",
    "    \n",
    "    # if claims or generated is empty, skip\n",
    "    df.at[index, \"f_deberta\"], df.at[index, \"f_deberta_mean\"] = focus(gold_claims, claims)\n",
    "    df.at[index, \"c_deberta\"], df.at[index, \"c_deberta_mean\"] = focus(claims, gold_claims)\n",
    "    df.at[index, \"r_deberta\"], df.at[index, \"r_deberta_mean\"] = focus(claims, claims, same=True)\n",
    "    df.at[index, \"f_align\"], df.at[index, \"f_align_mean\"] = focus_alignscore(gold_claims, claims)\n",
    "    df.at[index, \"c_align\"], df.at[index, \"c_align_mean\"] = focus_alignscore(claims, gold_claims)\n",
    "    df.at[index, \"r_align\"], df.at[index, \"r_align_mean\"] = focus_alignscore(claims, claims, same=True)\n",
    "    \n",
    "    # break at 100\n",
    "    # save df to outfile\n",
    "    if True or index % 100 == 0:\n",
    "        df.to_json(outfile, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factsumm import FactSumm\n",
    "from factsumm.utils.utils import qags_score\n",
    "\n",
    "factsumm = FactSumm()\n",
    "\n",
    "def focus(gold_claims: list[str], pred_claims: list[str], verbose: bool = False, same=False) -> float:\n",
    "    try:\n",
    "        if isinstance(factsumm.qg, str) or isinstance(factsumm.qa, str) or isinstance(factsumm.ner, str):\n",
    "            factsumm.extract_qas(\"b\", \" \".join(pred_claims), verbose=False, device=\"cuda:0\")\n",
    "                \n",
    "        # gold_entities = factsumm.ner(gold_claims)\n",
    "        pred_entities = factsumm.ner(pred_claims)\n",
    "        Q = factsumm.qg(pred_claims, pred_entities)\n",
    "\n",
    "        gold_answers = factsumm.qa(\" \".join(gold_claims), Q)\n",
    "        pred_answers = factsumm.qa(\" \".join(pred_claims), Q)\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            factsumm._print_qas(\"gold\", gold_answers)\n",
    "            factsumm._print_qas(\"pred\", pred_answers)\n",
    "\n",
    "        focus = qags_score(gold_answers, pred_answers)\n",
    "        if verbose:\n",
    "            print(f\"QAGS Score: {focus}\\n\")\n",
    "\n",
    "        return focus, pred_entities, Q, gold_answers, pred_answers\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.nan, [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5,\n",
       " [[{'word': 'R. Kelly', 'entity': 'PERSON', 'start': 0, 'end': 8},\n",
       "   {'word': '40 million', 'entity': 'CARDINAL', 'start': 14, 'end': 24}],\n",
       "  [{'word': 'R. Kelly', 'entity': 'PERSON', 'start': 0, 'end': 8}],\n",
       "  [{'word': 'R. Kelly', 'entity': 'PERSON', 'start': 0, 'end': 8},\n",
       "   {'word': 'one', 'entity': 'CARDINAL', 'start': 27, 'end': 30},\n",
       "   {'word': 'the United States', 'entity': 'GPE', 'start': 68, 'end': 85}]],\n",
       " [{'question': 'Who sold 40 million albums?', 'answer': 'R. Kelly'},\n",
       "  {'question': 'How many albums did Kelly sell?', 'answer': '40 million'},\n",
       "  {'question': 'Who is a musician?', 'answer': 'R. Kelly'},\n",
       "  {'question': 'Who was one of the best selling artists in the US?',\n",
       "   'answer': 'R. Kelly'},\n",
       "  {'question': 'How many times has Kelly been rated as a best selling artist in the US?',\n",
       "   'answer': 'one'},\n",
       "  {'question': 'Where was R. Kelly a well-known artist?',\n",
       "   'answer': 'the United States'}],\n",
       " [{'question': 'Who sold 40 million albums?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': '<unanswerable>'},\n",
       "  {'question': 'How many albums did Kelly sell?',\n",
       "   'answer': '40 million',\n",
       "   'prediction': '<unanswerable>'},\n",
       "  {'question': 'Who is a musician?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': 'R. Kelly'},\n",
       "  {'question': 'Who was one of the best selling artists in the US?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': 'R. Kelly'},\n",
       "  {'question': 'How many times has Kelly been rated as a best selling artist in the US?',\n",
       "   'answer': 'one',\n",
       "   'prediction': '<unanswerable>'},\n",
       "  {'question': 'Where was R. Kelly a well-known artist?',\n",
       "   'answer': 'the United States',\n",
       "   'prediction': 'the United States'}],\n",
       " [{'question': 'Who sold 40 million albums?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': 'R. Kelly'},\n",
       "  {'question': 'How many albums did Kelly sell?',\n",
       "   'answer': '40 million',\n",
       "   'prediction': '40 million'},\n",
       "  {'question': 'Who is a musician?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': 'R. Kelly'},\n",
       "  {'question': 'Who was one of the best selling artists in the US?',\n",
       "   'answer': 'R. Kelly',\n",
       "   'prediction': 'R. Kelly'},\n",
       "  {'question': 'How many times has Kelly been rated as a best selling artist in the US?',\n",
       "   'answer': 'one',\n",
       "   'prediction': '40 million'},\n",
       "  {'question': 'Where was R. Kelly a well-known artist?',\n",
       "   'answer': 'the United States',\n",
       "   'prediction': 'United States'}])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factsumm.utils.utils import qags_score\n",
    "focus(df[\"generated\"][0],df[\"claims\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
